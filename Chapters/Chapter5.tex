\chapter{Bi-directional dual arm object handover}


\paragraph*{Abstract}

\clearpage
\section{Introduction}

Learning Continuous Human-Robot Interactions from Human-Human Demonstrations ~\cite{vogt2017learning}

chapter 1 of robot dynamics and control book~\cite{spong2008robot}

--perform fluid object handover in a dynamic settings

--Design a planner considering —humans field of view, attention, preferences (left/right handed, etc),

--current state (sleeping, sitting, working etc) and the robot’s field of view, kinematics and dynamics



\clearpage
\section{Bi-directional Handover}

we take similar approach as one mentioned in~\cite{medina2016human} and treat handover as one stage cycle to perform fluid and intuitive handover between human and humanoid robot co-worker.

We formulate the bi-directional human-robot handover as one shot object handover from different starting hand poses (positions and orientations) along with hand speed and trajectories (of the human). The robot start from half-site and take the object that is handed by the human in one shot (and vice-versa). A person takes an object and hands it seriously to the robot that must take it (and vice-versa). The human start and try to give the object from different positions and orientations w.r.t the robot.

From onwards we will refer the routine of object handover between human and humanoid robot as \textit{cycle}, such that object handover from human to humanoid robot is one \textit{cycle} and return of object from humanoid robot to human as another \textit {cycle}. Therefore, it takes two full handover \textit{cycle}s to return the object back to where it started.

two important key features that we want to focus during human humanoid robot object handover is the time of handover and pose of handover.

block diagram here like~\cite{li2015predicting} -- done


\subsection{Variable object(s)}


\subsection{Handover Procedure}\label{fig:Pred_blockDiag}
This section focuses on simplified explanation of position prediction model using basic block diagram structure.

\begin{figure}[htp]
	\centering
	% Define block styles
	\tikzstyle{line} = [draw, -latex']
		
	\tikzstyle{cloud} = [draw, ellipse,fill=blue!20, text width=6em, text centered, node distance=4cm,  inner sep=0pt, minimum height=2em]
	
	\tikzstyle{startstop} = [rectangle, text centered, rounded corners, minimum width=3cm, minimum height=1cm, draw=black, node distance=4cm, fill=red!30]

	\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=-70,text centered, text width = 1cm,minimum height=1cm, minimum width=2cm, node distance=4cm, draw=black, fill=blue!30]
	
	\tikzstyle{process} = [rectangle, text centered, minimum width=1cm, minimum height=1cm, draw=black, fill=orange!30]

	\tikzstyle{block} = [rectangle, draw, fill=green!20, text width=10em, text centered, node distance=4cm, minimum height=4em]
	
	\tikzstyle{decision} = [diamond, draw, fill=yellow!20, text width=6em, text badly centered, node distance=4cm, inner sep=0pt]



	\begin{tikzpicture}[node distance = 3cm, auto]
	% Place nodes
	
	\node [process] (or) {or};
	
	\node [cloud, left of=or] (subj) {human has object};
	\node [cloud, right of=or] (robot) {robot has object};

	\node [startstop, below of=or, node distance = 2cm, auto] (start) {start handover procedure};
	
	\node [block, below of=start, node distance=2cm] (init) {initialize prediction model};
	\node [io, left of=init] (in1) {robot Ef pose};
	\node [io, right of=init] (in2) {human's hand pose};
	
	\node [block, below of=init, node distance = 3cm, auto] (relativePos) {observe human's hand pose relative to robot Ef pose};
	
	\node [decision, below of=relativePos] (within) {has human's hand arrived within robot constraint workspace?};
	\node [block, left of=within, node distance=5cm, text width=5em] (wait) {wait \& head-track object};
	\node [block, below of=within, node distance=4cm] (moveEf) {move robot Ef to predicted position};
	
	\node [decision, below of=moveEf, node distance = 4cm] (ifClose) {is human hand close enough to robot?};
	
	\node [startstop, below of=ifClose, node distance=3cm] (event) {stop motion, handover object};
	
	
%	\node [startstop, below of=ifClose, node distance=3cm] (stop) {stop};
	
	
	% Draw edges
	\path [line] (subj) -- (or);
	\path [line] (robot) -- (or);
	\path [line] (or) -- (start);
	\path [line] (start) -- (init);
	
	\path [line,dashed] (in1) -- (init);
	\path [line,dashed] (in2) -- (init);
	\path [line] (init) -- (relativePos);
	\path [line] (relativePos) -- (within);

	\path [line] (wait) |- (relativePos);
	\path [line] (within) -- node [near start] {no} (wait);
	\path [line] (within) -- node {yes} (moveEf);
	
	\path[line] (moveEf) -- (ifClose);
	\path [line] (ifClose) -- node [near start] {no} (moveEf);
	\path [line] (ifClose) -- node [near start] {yes} (event);
	
%	\path [line] (ifClose) -- node {no}(stop);
	
%	\path [line,dashed] (in2) |- (ifClose);
	\end{tikzpicture}
	\caption{Overview of human humanoid robot bi-directional handover procedure}
\end{figure}



\clearpage
\section{Notation and Terminology}
Let $\mathcal{M}$ (Mocap) and $\mathcal{R}$ (Robot) be the two fixed frames with same orientation that denote both Cartesian coordinate systems and Pl\"ucker coordinate systems denoted by $X$ in the Euclidean space. Both $\mathcal{M}$ and $\mathcal{R}$ are defined by their position and orientation of a Cartesian frame, such that ${}^MX_R$ denotes the Pl\"ucker coordinate transform which depends only on the position and orientation of frame $\mathcal{M}$ relative to frame $\mathcal{R}$ (see Fig~\ref{fig:frames}).

We used 6D spatial vectors to express the pose of human hand and robot end-effector. We therefore adopt a 6D notation based on spatial vectors, which is explained in Chapter 2 of~\cite{featherstone2014rigid}. 


\begin{figure}[ht]
	\centering{\includegraphics[width=1\columnwidth]{Chapters/c5-plots/frame}}
	\caption{$\mathcal{M}$ and $\mathcal{R}$ Cartesian coordinate systems.}
	\label{fig:frames}
\end{figure}


\begin{itemize}
	\item using QP Orientation Task and Position Task (see~\ref{qpTasks}), we get robot end-effector current orientation ${{}^{ef}\mathcal{O}_R} \in \mathbb{R}^{3\times3}$ and current position ${{}^{ef}\mathcal{P}_R} \in \mathbb{R}^{3}$ respectively in the $\mathcal{R}$ frame, therefore,
	\begin{gather}\label{X_R_ef}
		{}^{ef}{X}_R =
		\left[\begin{array}{cc}
		{}^{ef}\mathcal{O}_R & {}^{ef}\mathcal{P}_R
		\end{array}\right]
	\end{gather}
	
	\item Likewise, ${{}^{h}\mathcal{O}_M} \in \mathbb{R}^{3\times3}$ and ${{}^{h}\mathcal{P}_M} \in \mathbb{R}^{3}$, denotes the human's hand $h$ orientation and position respectively in the $\mathcal{M}$ frame, obtained from the $\mathcal{L}$ shape body (see Section~\ref{hand_orientation}),
	\begin{gather}\label{X_M_h}
		{}^{h}{X}_M =
		\left[\begin{array}{cc}
		{}^{h}\mathcal{O}_M & {}^{h}\mathcal{P}_M \\
		\end{array}\right]
	\end{gather}

	\item ${{}^{h}\mathcal{O}_{ef}} \in \mathbb{R}^{3\times3}$ and ${{}^{h}\mathcal{P}_{ef}} \in \mathbb{R}^{3}$, denotes the human's hand $h$ orientation and position respectively in relative to the robot's local $ef$ (end-effector) frame,
	\begin{gather}\label{X_ef_h}
		{}^{h}{X}_{ef} =
		\left[\begin{array}{cc}
		{}^{h}\mathcal{O}_{ef} & {}^{h}\mathcal{P}_{ef} \\
		\end{array}\right]
	\end{gather}
	
	\item For convenience, we formulate the problem with a common origin {\it O}, such that $\mathcal R \equiv M$ (both frames are located between the feet of robot HRP-2Kai), we can get human's hand pose $\mathsf{w.r.t.}$ or relative to robot's end-effector
%	
	\begin{equation}\label{X_ef_h1}
		{}^{h}{X}_{ef} = {}^{h}{X}_{M}  {}^{ef}{X_{R}}^{-1}
	\end{equation}
	\item otherwise, when $\mathcal R \neq M$

	\begin{equation}\label{X_ef_h2}
	{}^{h}{X}_{ef} = {}^{h}{X}_{M}  {}^{M}{X}_R  {}^{ef}{X_{R}}^{-1}
	\end{equation}
	
\end{itemize}


\clearpage
\section{Robot Control - QP Tasks}\label{qpTasks}
ladder climbing paper~\cite{ladder-HRP-2Kai}

\subsection{Position Task}\label{positionTask}

\subsection{Orientation Task}\label{orientationTask}

\subsection{COM Task}\label{comTask}

\subsection{Vector look at Head Task}\label{HeadTask}

\subsection{Contact Task}\label{contact}
refer chapter 11 of \cite{featherstone2014rigid}
constraint motion

Tasks::qp::ContactSpeedConstr(timeStep)
QPContactConstr.h/cpp

\begin{lstlisting}[language=C++,basicstyle=\footnotesize, caption={QPContactConstr}]

void ContactSpeedConstr::update(const std::vector<rbd::MultiBody>& mbs,
const std::vector<rbd::MultiBodyConfig>& mbcs,
const SolverData& data)
{
using namespace Eigen;

A_.block(0, 0, nrEq_, totalAlphaD_).setZero();
b_.head(nrEq_).setZero();
// J_i*alphaD + JD_i*alpha = 0

int index = 0;
for(std::size_t i = 0; i < cont_.size(); ++i)
{
ContactData& cd = cont_[i];
int rows = int(cd.dof.rows());

for(std::size_t j = 0; j < cd.contacts.size(); ++j)
{
ContactSideData& csd = cd.contacts[j];
const rbd::MultiBody& mb = mbs[csd.robotIndex];
const rbd::MultiBodyConfig& mbc = mbcs[csd.robotIndex];

// AEq = J_i
sva::PTransformd X_0_p = csd.X_b_p*mbc.bodyPosW[csd.bodyIndex];
const MatrixXd& jacMat = csd.jac.jacobian(mb, mbc, X_0_p);
dofJac_.block(0, 0, rows, csd.jac.dof()).noalias() =
csd.sign*cd.dof*jacMat;
csd.jac.fullJacobian(mb, dofJac_.block(0, 0, rows, csd.jac.dof()),
fullJac_);
A_.block(index, csd.alphaDBegin, rows, mb.nrDof()).noalias() +=
fullJac_.block(0, 0, rows, mb.nrDof());

// BEq = -JD_i*alpha
Vector6d normalAcc = csd.jac.normalAcceleration(
mb, mbc, data.normalAccB(csd.robotIndex), csd.X_b_p,
sva::MotionVecd(Vector6d::Zero())).vector();
Vector6d velocity = csd.jac.velocity(mb, mbc, csd.X_b_p).vector();
b_.segment(index, rows).noalias() -= csd.sign*cd.dof*(normalAcc +
velocity/timeStep_);
}
index += rows;
}
}
\end{lstlisting}


\clearpage
\section{Human Hand Orientation Model}\label{hand_orientation}
To get the orientation of human's hand, we placed a rigid body with a shape similar to alphabet $L$ on wrist of human's hand(s) as shown in Fig~\ref{fig:lshapes}. The three mocap markers $A, B$ and $C$ make up the three vertices of $\mathcal{L}$ shape body, such that during object handover scenario, vector $\vec{AB}^{\,}$ along the longer side and vector $\vec{BC}^{\,}$ along the shorter side are set to be parallel with the $X$-axis and $Y$-axis of the mocap frame $\mathcal{M}$ respectively. The vectors $\vec{AB}^{\,}$ and $\vec{BC}^{\,}$ are orthogonal to each other. For simplicity, let $\hat{x}$ be the unit vector, which is parallel and along the $X$-axis and likewise, unit vector $\hat{y}$ is parallel and along the $Y$-axis of the mocap frame $\mathcal{M}$, such that

\begin{figure}[ht]
	\centering{\includegraphics[width=1\columnwidth]{Chapters/c5-plots/lshpe-lt-rt}}
	\caption{$\mathcal{L}$ shape rigid body on the human's hand(s)}
	\label{fig:lshapes}
\end{figure}

\begin{equation*}
\hat{x} = \frac{\vec{AB}^{\,}}{\norm{\vec{AB}^{\,}}}
\end{equation*}

\begin{equation*}
\hat{y} = \frac{\vec{BC}^{\,}}{\norm{\vec{BC}^{\,}}}
\end{equation*}

\paragraph*{}
Let ${}^{h}\mathcal{O}_{M}$ in equation (\ref{X_M_h}) be the column rotation matrix representing the human's hand orientation in the mocap frame $\mathcal{M}$ and since $\hat{x} \in \mathbb{R}^{3}$ and $\hat{y} \in \mathbb{R}^{3}$ are the two orthogonal unit vectors of an $\mathcal{L}$ shape rigid body on human's each hand (Fig~\ref{fig:lshapes}), the cross product of them would result in another unit vector $\hat{z} \in \mathbb{R}^{3}$ which is orthogonal to both $\hat{x}$ and $\hat{y}$. The direction of the cross product would be given by equation (\ref{cross}) using right-hand rule.


\begin{equation}\label{cross}
\hat{z} = \hat{x} \times \hat{y}
\end{equation}


\paragraph*{}
To get the orientation of human's hand we used these unit vectors $\hat{x}, \hat{y}$ and $\hat{z}$ as columns of the rotation matrix~\cite{evans2001rotations, altmann2005rotations, jia2017rotation} ${}^{h}\mathcal{O}_{M}$ in equation (\ref{rotationmatrix}), such that the unit vectors $\hat{x}, \hat{y}$ and $\hat{z}$ represents human's hand orientation around the $roll-pitch-yaw$ axes respectively.

\begin{equation}\label{rotationmatrix}
{}^{h}\mathcal{O}_{M} = 
\left\{\begin{array}{cccc}
\hat{x.x} & \hat{y.x} & \hat{z.x} \\
\hat{x.y} & \hat{y.y} & \hat{z.y} \\
\hat{x.z} & \hat{y.z} & \hat{z.z}
\end{array}\right\}_{3\times 3}
\end{equation}



\clearpage
\section{Human Hand Position Prediction Model}\label{prediction_model}
Algorithm~\ref{positionalgo} explains the procedure of predicting human's hand position and estimating handover location. Inputs to the prediction model are robot left end-effector pose $\mathcal{}^{ef}{X}_R$ (eq~\ref{X_R_ef}) in the robot frame and human's hand mocap markers positions ${}^{h}\mathcal{P}_M$ (eq~\ref{X_M_h}) in the mocap frame of reference.

\paragraph*{}
Note that for simplicity, we formulated the problem with a common origin {\it O}, such that frame $\mathcal R \equiv M$, also from onwards by ${}^{h}\mathcal{P}_M$ we meant position of the point $A$ on the $\mathcal{L}$ shape body as the human's hand marker position as shown in Fig~\ref{fig:lshapes}.


\paragraph*{}
The prediction model behavior can be tuned by two initially required constant time periods, $t_{observe}$ ---a predefined time period required to observe the motion of human's hand and $t_{predict}$ ---required to predict the human's hand position in advance.

\paragraph*{}
In order for handover between human and robot HRP-2Kai to be smooth and fluent, the robot needs to estimate the position of handover location in advance during both cases when robot act as receiver or when robot acts as giver, therefore we formulate the motion of human's hand as a constant velocity based linear motion model (see Algorithm~\ref{positionalgo}) to predict the handover position continuously. The robot observes human's hand for a predefined time period $t_{observe}$ and estimates the human's hand direction and position based on the human's hand velocity using equations (\ref{predictVel}) and (\ref{predict}) during the predefined time period. 

\begin{equation} \label{predictVel}
{}^{h}\mathcal{\bar{V}}_{M} = \frac{1}{t_{observe}}{\sum_{j=1}^{j=t_{observe}} ({}^{h}\mathcal{P}_{M}(j)-{}^{h}\mathcal{P}_{M}(j-1))/dt }
\end{equation}

where, $dt$ is controller run-time, in our case it is 5ms.

\begin{equation} \label{predict}
{}^{h}\mathcal{P}_M(t_{predict}) = {}^{h}\mathcal{\bar{V}}_{M} \cdot t_{predict}  + {}^{h}\mathcal{P}_{M}(t_{observe})
\end{equation}

\paragraph*{}
The prediction model then updates and converges itself over time and in doing so update the robot's end-effector position towards the human's hand, upon condition if updated position is within the end-effector's constraint workspace, see Fig XXX.

\paragraph*{}
Using equations~\ref{rotationmatrix} and~\ref{predict} we get the orientation and translation components of ${}^{h}{X}_M$ in~\ref{X_M_h}, and recalling equations~\ref{X_ef_h1} and~\ref{X_ef_h2}, the updated translation component of pl\"ucker transformation ${}^{h}{X}_{ef}$, which provides the predicted position of human's hand with respect to robot end-effector in the robot coordinate system is given by~\ref{X_ef_ht}, the relative orientation component of ${}^{h}{X}_{ef}$ is discussed in next Section~\ref{relOri}.

\begin{equation}\label{X_ef_ht}
{}^{h}{X}_{ef}(t_{predict}) =  {}^{h}{X}_M  {}^{M}{X}_R {{}^{ef}{X}_R}^{-1}
\end{equation}

where, ${}^{M}{X}_R$ is the Pl\"ucker coordinate transform frame $\mathcal{M}$ relative to frame $\mathcal{R}$, if $\mathcal R \neq M$.

\paragraph*{}
Finally, the handover location is estimated as the human's hand predicted position when the following conditions satisfy (\ref{min_posErr_vel}), that is when the human's hand velocity is minimum and robot end-effector is closest to the human's hand.

\begin{equation}\label{min_posErr_vel}
\left.\begin{aligned}
\norm{{}^{h}\mathcal{\bar{V}}_{M}}& <= 1e^{-2} \\
\norm{({}^{h}\mathcal{P}_{ef}(t_{predict}) - {}^{ef}\mathcal{P}_{R})} & <= 1e^{-2}
\end{aligned}
\right\}
\end{equation}


\subsection{Algorithm: Position Prediction Model}

\begin{algorithm}[H] \label{positionalgo}
	\DontPrintSemicolon
	
	\KwInput{$\mathcal{}^{M}{X}_R, {}^{ef}{X}_R, mocapData$}
	\KwOutput{${}^{h}wp_{ef}$ \tcp*{predicted waypoints}  }
	\KwData{Initial require: $t_{observe}=10ms, t_{predict}=100ms, i=dt=5ms$}
	
	
	\textit{$\textbf{i+=dt}$} \tcp*{increments as per controller run-time (dt)}
	
	\If{$(i\%t_{observe})==0$}
	{
		\For{$j = 1 \textrm{ to } t_{observe} $}
		{
			${}^{h}\mathcal{P}_M= \textit{mocapData}.handMarker(i-t_{observe})+j)$	
		}
		
		${}^{h}\mathcal{\bar{V}}_{M} = \frac{1}{t_{observe}}{\sum_{j=1}^{j=t_{observe}} ({}^{h}\mathcal{P}_{M}(j)-{}^{h}\mathcal{P}_{M}(j-1))/dt }$\newline 
		
		\tcc{predict human's hand handover position at $t_{predict}$}
		${}^{h}\mathcal{P}_M(t_{predict}) = {}^{h}\mathcal{\bar{V}}_{M} \cdot t_{predict}  + {}^{h}\mathcal{P}_{M}(t_{observe})$ %\newline % \times 0.005
		
		
		${}^{h}{X}_M= \begin{bmatrix} {}^{h}\mathcal{O}_{M} &  {}^{h}\mathcal{P}_M	\end{bmatrix}$ \newline
		
		\tcc{transform handover position relative to robot end-effector}	
		${}^{h}{X}_{ef}(t_{predict}) =  {}^{h}{X}_M  {}^{M}{X}_R {{}^{ef}{X}_R}^{-1}$ \newline
		
		
		\tcc{\textit{way points between human hand and robot end-effector handover location}}
		\SetKwFunction{FMain}{generateWp}
		\SetKwProg{Fn}{Function}{:}{}
		\Fn{\FMain{$ {}^{ef}\mathcal{P}_{R}, {}^{h}\mathcal{P}_{ef}(t_{predict}), t_{predict} $}}
		{
			\For{$k = 0 \textrm{ to } t_{predict} $}
			{
				${}^{h}wp_{ef}(k) = [{}^{h}\mathcal{P}_{ef}(t_{predict}) - {}^{ef}\mathcal{P}_{R}] . (\frac{k}{t_{predict}})  + {}^{ef}\mathcal{P}_{R} $ 
			}	
			\textbf{return} $ {}^{h}wp_{ef} $
		}
	}
	\caption{linear prediction model - Position}
\end{algorithm}


\clearpage
\section{Relative Orientation Model}\label{relOri}
The translation component of ${}^{ef}{X}_R $ in the equation (\ref{X_R_ef}) is updated based on the predicted position of human's hand (see Section~\ref{prediction_model}) while the orientation component is updated based on the relative orientation of human's hand (see Section~\ref{hand_orientation}).

\begin{figure}[ht]
	\centering{\includegraphics[width=.7\linewidth]{Chapters/c5-plots/rviz_robot_lt_hand_obj_frame}}
	\caption{Robot HRP-2Kai (left end-effector) holding object with fixed orientation during handover in the robot frame $\mathcal{R}$}
	\label{fig:robot_lt_hand_obj}
\end{figure} 

\paragraph*{}
Since our robot HRP-2Kai lacks conventional anthropomorphic hands, but instead it has gripper alike hands~\cite{kaneko2015humanoid, stasse2019overview} mainly to increase the manipulability. For relative orientation model to work, we were required to know the fixed initial orientation of the robot end-effector, in which object handover is feasible between human and robot HRP-2Kai, irrespective of the human's hand orientation, therefore consider a possible  scenario where the robot end-effector orientation is fixed throughout the handover cycle (transfer of object from human to robot and return to human), irrespective of the condition where robot is a giver or receiver. Let ${{}^{efInit}\mathcal{O}_R}$ denote the initial and fixed orientation of the robot end-effector in the robot frame $\mathcal{R}$ during such human robot object handover scenarios as shown in Fig~\ref{fig:robot_lt_hand_obj}, and Fig~\ref{fig:robot_lt_hand_2layers}, where, ${{}^{efInit}\mathcal{O}_R}$ $\subset$ ${{}^{ef}\mathcal{O}_R}$.

\begin{figure}[ht]
	\centering{\includegraphics[width=.7\linewidth]{Chapters/c5-plots/rviz_robot_lt_hand_obj-2layer-empty}}
	\caption{Robot HRP-2Kai (left end-effector) fixed orientation during two handover trials in the robot frame $\mathcal{R}$}
	\label{fig:robot_lt_hand_2layers}
\end{figure} 


In order to correctly transform desired human's hand orientation into robot end-effector frame, we further utilize the equation (\ref{X_ef_ht}). We take advantage of the QP's Orientation Task (see Subsection~\ref{orientationTask}) and calculate the task error $e$ between \textit{current} human's hand and \textit{fixed} robot end-effector orientations,
the resulting orientation from the Orientation Task is the desired robot end-effector orientation relative to the human's hand orientation, see Fig~\ref{fig:robot_lt_orientations}, therefore, using fixed orientation component  ${{}^{efInit}\mathcal{O}_R}$, of equation (\ref{X_R_ef}) and current human's hand orientation ${}^{h}\mathcal{O}_M$ from equation (\ref{rotationmatrix}) derived in Section~\ref{hand_orientation}, we further modify equation (\ref{X_ef_ht}).

\begin{figure}[ht]
	\centering{\includegraphics[width=1\columnwidth]{Chapters/c5-plots/robot_lt_orientations}}
	\caption{Robot HRP-2Kai (left end-effector) holding object in multiple possible orientations during handover trials in the robot frame $\mathcal{R}$}
	\label{fig:robot_lt_orientations}
\end{figure} 


\begin{gather}\label{X_efinit_hori}
\left[\begin{array}{cc}
{}^{h}\mathcal{O}_{ef} & {}^{h}\mathcal{P}_{ef}
\end{array}\right] =
\left[\begin{array}{cc}
\bf{{}^{h}\mathcal{O}_M} & {}^{h}\mathcal{P}_M
\end{array}\right]
\left[\begin{array}{cc}
{}^{M}\mathcal{O}_R & {}^{M}\mathcal{P}_R
\end{array}\right]
\left[\begin{array}{cc}
\bf{{}^{efInit}\mathcal{O}_{R}} & {}^{ef}\mathcal{P}_{R}
\end{array}\right]^{-1}
\end{gather}

Computing orientation task Err from~\cite{ladder-HRP-2Kai}


\clearpage
\section{Handover Force Control Scheme}

HRP-2Kai is equipped with 6-axis \textit{wrist} force sensor on both hands, capable of precisely sensing interaction forces and torques with the environment along $x, y, z$ axes of the local coordinate frame of the sensor. We designed a force control scheme which enable the robot end-effector to interact with the object without knowing object's mass in advance.

In order to understand the force control scheme, let $\mathcal{\vec{F}}$ and $\vec{f}$ denote the 3D \textit{gravity-free} force vectors in the robot frame $\mathcal{R}$ and sensor local frame respectively. During human to robot handover cycle when human holds the object, in such a situation due to any acceleration of the robot end-effector while approaching towards the object, a wrist force sensor would only read inertial forces, which we termed $F_{zero}$ as force sensor offset reading. To get more information on how we transformed force sensor data into into world frame and without gravity free reading please see Appendix~\ref{handover}.

One of the main problem that we focused here is related to the \textit{timing} at which the robot should close its gripper in order to hold the object, if robot tries to close too early to grasp the object when human subject is not ready it can lead to collision resulting in accident or if robot waits too long then it just depreciate the purpose of this study.

We had to find equilibrium for the robot to know the appropriate timing at which the handover should occur, which would be natural and intuitive to the eyes of human. We came up with two methods to tackle this problem, one highlights simplicity and other highlights efficiency but when used together we get a reliable and safest solution possible under such scenario.


\subsection{Mocap marker based}
Without actual haptic feedback information from the robot end-effector (gripper surface), we had to rely on the wrist force sensor and mocap markers data of the robot end-effector, therefore we used mocap makers in conjunction with the force feedback from the wrist sensor of HRP-2Kai robot to know whether the object is within its vicinity to grasp or not. Mocap makers were crucial to know the relative position of object from the human hand holding it and the humanoid robot end-effector(s).




\subsection{Surface wrench based}
the relative position of object with respect to its end-effector gripper surface without HRP-2Kai haptic feedback,



\subsection{Algorithm: Gripper Force Control}

\begin{algorithm}[H]
	\DontPrintSemicolon
	
	\KwInput{$\mathcal{F}_{w}$\tcp*{EF wrist worldWrenchWithoutGravity}}
	\KwOutput{$\mathcal{F}_{pull}, T_{new}$ \tcp*{Pull force, new threshold based on object mass\newline} }
	\textit{$\textbf{i++}$} \tcp*{increments as per controller run-time (5ms)}
	
	\If{human hand is near robot}
	{
		\tcc{when human holds the object}
		\If{$\mathcal{F}_{w}.norm()<1.0 $\tcp*{gripper is empty}}
		{
			$\mathcal{F}_{zero}= \mathcal{F}_{w}$\tcp*{wrench offset}
		}
%		\ElseIf{$\mathcal{F}_{w}.norm()>2.0 $}
%		{
%			$\mathcal{F}_{load}= \mathcal{F}_{w}$\tcp*{wrench with object}
%		}
		\tcc{when ROBOT holds the object}
		\SetKwFunction{FMain}{CheckPullForce}
		\SetKwProg{Fn}{Function}{:}{}
		\Fn{\FMain{$axis\forall${x,y,z}}}
		{
			$objectMass = \mathcal{F}_{load}/9.81 $\tcp*{get object mass}
			
			$\mathcal{F}_{inert} = objectMass * efAce  $\tcp*{$efAce$ using gripper markers}
			
			$\mathcal{F}_{pull} = \vert{(\mathcal\vert{{F}}_{w} - \mathcal\vert{{F}}_{inert} - \mathcal\vert{{F}}_{zero}) }$
			
			$T_{new} = \mathcal{F}_{load} + T_{old}$\tcp*{$T_{old}$ set to min by user}
			
			\If{$\mathcal{F}_{pull} > T_{new}$ {$\forall${x,y,z}}}
			{
				$openGripper, release object$\tcp*{release object}
			}
		}
	}
	\Else
	{
		\If{$(i \% 200)$}
		{
			$\mathcal{F}_{load} = \vert{(\mathcal{F}_{w} - \mathcal{F}_{zero})} $\tcp*{$\forall${x,y,z} average over time}
		}
	}
	\caption{Force Based Gripper Controller}
\end{algorithm}

\subsection{Finite State Machine}

explain me here

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=5cm, auto, scale = 1, ->,>=stealth, line width=3pt]
	
	\tikzstyle{cloud} = [draw,ultra thick, circle ,fill=white!20, text width=5em, text centered, node distance=10em, auto, inner sep=0pt, minimum height=2em]
	
	\tikzstyle{blank} = [circle ,fill=white!20, text width=5em, text centered, node distance=10em, auto, inner sep=0pt, minimum height=2em]
	
	\tikzstyle{line} = [draw, -latex', inner sep=1pt, minimum height=2em, node distance=10em]
	
	\node[cloud, initial, accepting] (human has object){human has object?};
	\node[cloud, node distance=4cm] (human hand is near?) [right of=human has object] {is object near robot?};
	\node[cloud] (open gripper) [below of=human hand is near?] {open gripper};
	\node[cloud] (stop motion) [below of=open gripper] {stop motion};
	\node[cloud] (object inside gripper?) [right of=open gripper] {object inside gripper?};
	\node[cloud] (close gripper) [below of=object inside gripper?] {close gripper};
	\node[cloud,accepting] (robot has object) [right of=close gripper] {robot receives object};		
	\node[cloud] (go rest pose) [below of=robot has object] {go to rest pose};
	\node[cloud] (start motion) [left of=go rest pose] {start motion};
	\node[cloud, node distance=3.8cm] (human hand is near again?) [left of=start motion] {human hand near object?};	
	\node[cloud] (pull handover object) [below of=human hand is near again?] {human pulls object?};
	\node[cloud, node distance=5cm] (open gripper release object) [right of=pull handover object] {gripper releases object};
	\node[cloud,accepting] (handover occurred) [below of=open gripper release object] {object returns to human};
	\node[cloud, node distance=5cm] (go rest pose2) [left of=handover occurred] {go to rest pose};
	
	\node[blank] (blank)[left of = go rest pose2]{};
	
	\path [line] (human has object) edge              node{1st cycle} (human hand is near?);
	\path [line] (human hand is near?) edge              node {} (open gripper);
	\path [line] (open gripper) edge              node {} (stop motion);
	\path [line] (open gripper) edge              node {} (object inside gripper?);
	\path [line] (object inside gripper?) edge   node {} (close gripper);
	\path [line] (close gripper) edge              node [near start, rotate=-45]{if false close} (open gripper);
	\path [line, node distance=6cm] (close gripper) edge node {}(robot has object);
	\path [line, node distance=6cm] (robot has object) edge node{} (go rest pose);
	
	\path [line] (go rest pose) edge              node {2nd cycle} (start motion);
	
	\path [line]  (start motion)edge               node {} (human hand is near again?);
	
	\path [line]  (human hand is near again?)edge               node {$F_{object}$}  (pull handover object);
	
	\path [line] (human hand is near again?) edge              node {} (stop motion);
	
	\path [line]  (pull handover object)edge               node {$F_{pull} >Th_{new}$} (open gripper release object);
	
	\path [line]  (open gripper release object)edge      node {}(handover occurred);
	\path [line]  (handover occurred)edge               node {}(go rest pose2);
	
	\path [line, dashed]  (go rest pose2) -|               node {\textbf{restart handover}}(human has object);
	
	
	\end{tikzpicture}
	\caption{Overview of human humanoid robot object handover Finite-State-Machine (FSM)}
\end{figure}


\clearpage
\section{both hands indiviudal- adding another hand}

the switching is based on the function which  uses hysteresis to compute relative position of object between human's hand and robot end-effectors 

\clearpage
\section{both hands together- using hands together}

\subsection{Variable object(s)}

\subsection{Constraint motion}

\subsection{Force Control changes}



\clearpage
\section{add a step-walk \& native stablizer}

\clearpage
\section{Repeat handover with step-walk}

\clearpage
\section{Experiments}

\clearpage
\section{Quantitative analysis}

\clearpage
\section{Results}

\clearpage
\section{Discussion}
%----an idea --have velocity proportional to the distance between robot ef and subj w.r.t. handover location






%the maths of rotation matrix look for relative rotation
%http://www.fastgraph.com/makegames/3drotation/

%page 11, 12  %http://web.cs.iastate.edu/~cs577/handouts/rotation.pdf

%wiki page ... {$https://en.wikipedia.org/wiki/Rotation_formalisms_in_three_dimensions$}

%3D to 2D rotation matrix reduction
%http://www.continuummechanics.org/rotationmatrix.html

%function ==direction-vector-to-rotation-matrix
%https://stackoverflow.com/questions/18558910/direction-vector-to-rotation-matrix

%vector direction link below
%${https://en.wikipedia.org/wiki/Direction_cosine}$

