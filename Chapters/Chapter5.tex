\chapter{Bi-directional dual arm object handover\newline}

\newpage
\section{Introduction*}
--perform fluid object handover in a dynamic settings

--Design a planner considering —humans field of view, attention, preferences (left/right handed, etc),

--current state (sleeping, sitting, working etc) and the robot’s field of view, kinematics and dynamics


\newpage
\section{Robot Control - QP Tasks}\label{qpTasks}

\subsection{Position Task}\label{positionTask}

\subsection{Orientation Task}\label{orientationTask}

\subsection{COM Task}\label{comTask}

\subsection{Vector look at Head Task}\label{HeadTask}

\subsection{Contact Task}\label{contact}



\newpage
\section{Bi-directional Handover}
We formulate the bi-directional human-robot handover as one shot object handover from different starting positions and hand speed and trajectories (of the human). The robot start from half-site and take the object that is handed by the subject in one shot (and vice-versa). A person takes an object and hands it seriously to the robot that must take it (and vice-versa). The subject start and try to give the object from different positions and orientations w.r.t the robot.

two important key features that we want to focus during human humanoid robot object handover is the time of handover and pose of handover.


\newpage
\section{Notation and Terminology}
Let $\mathcal{M}$ (Mocap) and $\mathcal{R}$ (Robot) be the two fixed frames that denote both Cartesian coordinate systems and Pl\"ucker coordinate systems denoted by $X$ in the Euclidean space. Both $\mathcal{M}$ and $\mathcal{R}$ are defined by their position and orientation of a Cartesian frame, such that ${}^MX_R$ denotes the Pl\"ucker coordinate transform which depends only on the position and orientation of frame $\mathcal{M}$ relative to frame $\mathcal{R}$\cite{featherstone2014rigid} (see Fig~\ref{fig:frames}).


\begin{figure}[h]
	\centering{\includegraphics[width=1\columnwidth]{Chapters/c5-plots/frame}}
	\caption{$\mathcal{M}$ and $\mathcal{R}$ Cartesian coordinate systems.}
	\label{fig:frames}
\end{figure}


\begin{itemize}
	\item using QP Orientation Task and Position Task (see~\ref{qpTasks}), we get robot end-effector current orientation ${{}^{ef}\mathcal{O}_R} \in \mathbb{R}^{3\times3}$ and current position ${{}^{ef}\mathcal{P}_R} \in \mathbb{R}^{3}$ respectively in the $\mathcal{R}$ frame, therefore,
	\begin{gather}\label{X_R_ef}
		{}^{ef}{X}_R =
		\left[\begin{array}{cc}
		{}^{ef}\mathcal{O}_R & {}^{ef}\mathcal{P}_R
		\end{array}\right]
	\end{gather}
	
	\item Likewise, ${{}^{h}\mathcal{O}_M} \in \mathbb{R}^{3\times3}$ and ${{}^{h}\mathcal{P}_M} \in \mathbb{R}^{3}$, denotes the subject's hand $h$ orientation and position respectively in the $\mathcal{M}$ frame, obtained from the $\mathcal{L}$ shape body (see Section~\ref{hand_orientation}),
	\begin{gather}\label{X_M_h}
		{}^{h}{X}_M =
		\left[\begin{array}{cc}
		{}^{h}\mathcal{O}_M & {}^{h}\mathcal{P}_M \\
		\end{array}\right]
	\end{gather}

	\item ${{}^{h}\mathcal{O}_{ef}} \in \mathbb{R}^{3\times3}$ and ${{}^{h}\mathcal{P}_{ef}} \in \mathbb{R}^{3}$, denotes the subject's hand $h$ orientation and position respectively in relative to the robot's local $ef$ (end-effector) frame,
	\begin{gather}\label{X_ef_h}
		{}^{h}{X}_{ef} =
		\left[\begin{array}{cc}
		{}^{h}\mathcal{O}_{ef} & {}^{h}\mathcal{P}_{ef} \\
		\end{array}\right]
	\end{gather}
	
	\item For convenience, we formulate the problem with a common origin {\it O}, such that $\mathcal R \equiv M$ (both frames are located between the feet of robot HRP2-Kai), we can get subject's hand pose $\mathsf{w.r.t.}$ or relative to robot's end-effector
%	
	\begin{equation}\label{X_ef_h1}
		{}^{h}{X}_{ef} = {}^{h}{X}_{M}  {}^{ef}{X_{R}}^{-1}
	\end{equation}
	\item otherwise, when $\mathcal R \neq M$

	\begin{equation}\label{X_ef_h2}
	{}^{h}{X}_{ef} = {}^{h}{X}_{M}  {}^{M}{X}_R  {}^{ef}{X_{R}}^{-1}
	\end{equation}
	
\end{itemize}




\newpage
\section{Subject's Hand Orientation Model}\label{hand_orientation}
To get the orientation of subject's hand, we placed a rigid body with a shape similar to alphabet $L$ on wrist of subject's hand(s) as shown in Fig~\ref{fig:lshapes}. The three mocap markers $A, B$ and $C$ make up the three vertices of $\mathcal{L}$ shape body, such that during object handover scenario, vector $\vec{AB}^{\,}$ along the longer side and vector $\vec{BC}^{\,}$ along the shorter side are set to be parallel with the $X$-axis and $Y$-axis of the mocap frame $\mathcal{M}$ respectively. The vectors $\vec{AB}^{\,}$ and $\vec{BC}^{\,}$ are orthogonal to each other. For simplicity, let $\hat{x}$ be the unit vector, which is parallel and along the $X$-axis and likewise, unit vector $\hat{y}$ is parallel and along the $Y$-axis of the mocap frame $\mathcal{M}$, such that

\begin{figure}[h]
	\centering{\includegraphics[width=1\columnwidth]{Chapters/c5-plots/lshpe-lt-rt}}
	\caption{$\mathcal{L}$ shape rigid body on the subject's hand(s)}
	\label{fig:lshapes}
\end{figure}

\begin{equation*}
\hat{x} = \frac{\vec{AB}^{\,}}{\norm{\vec{AB}^{\,}}}
\end{equation*}

\begin{equation*}
\hat{y} = \frac{\vec{BC}^{\,}}{\norm{\vec{BC}^{\,}}}
\end{equation*}

\paragraph*{}
Let ${}^{h}\mathcal{O}_{M}$ in equation (\ref{X_M_h}) be the column rotation matrix representing the subject's hand orientation in the mocap frame $\mathcal{M}$ and since $\hat{x} \in \mathbb{R}^{3}$ and $\hat{y} \in \mathbb{R}^{3}$ are the two orthogonal unit vectors of an $\mathcal{L}$ shape rigid body on subject's each hand (Fig~\ref{fig:lshapes}), the cross product of them would result in another unit vector $\hat{z} \in \mathbb{R}^{3}$ which is orthogonal to both $\hat{x}$ and $\hat{y}$. The direction of the cross product would be given by equation (\ref{cross}) using right-hand rule.


\begin{equation}\label{cross}
\hat{z} = \hat{x} \times \hat{y}
\end{equation}


\paragraph*{}
To get the orientation of subject's hand we used these unit vectors $\hat{x}, \hat{y}$ and $\hat{z}$ as columns of the rotation matrix~\cite{evans2001rotations, altmann2005rotations, jia2017rotation} ${}^{h}\mathcal{O}_{M}$ in equation (\ref{rotationmatrix}), such that the unit vectors $\hat{x}, \hat{y}$ and $\hat{z}$ represents subject's hand orientation around the $roll-pitch-yaw$ axes respectively.

\begin{equation}\label{rotationmatrix}
{}^{h}\mathcal{O}_{M} = 
\left\{\begin{array}{cccc}
\hat{x.x} & \hat{y.x} & \hat{z.x} \\
\hat{x.y} & \hat{y.y} & \hat{z.y} \\
\hat{x.z} & \hat{y.z} & \hat{z.z}
\end{array}\right\}_{3\times 3}
\end{equation}



\newpage
\section{Handover Position Prediction Model}\label{prediction_model}
Algorithm~\ref{positionalgo} explains the procedure of predicting subject's hand position and estimating handover location. Inputs to the prediction model are robot left end-effector pose $\mathcal{}^{ef}{X}_R$ (eq~\ref{X_R_ef}) in the robot frame and subject's hand mocap markers positions ${}^{h}\mathcal{P}_M$ (eq~\ref{X_M_h}) in the mocap frame of reference.

\paragraph*{}
Note that for simplicity, we formulated the problem with a common origin {\it O}, such that $\mathcal R \equiv M$, also from onwards by ${}^{h}\mathcal{P}_M$ we meant position of the point $A$ on the $\mathcal{L}$ shape body as the subject's hand marker position as shown in Fig~\ref{fig:lshapes}.


\paragraph*{}
The prediction model behavior can be tuned by two initially required constant time periods, $t_{observe}$ ---a predefined time period required to observe the motion of subject's hand and $t_{predict}$ ---required to predict the subject's hand position in advance.

\paragraph*{}
In order for handover between human subject and robot HRP2-Kai to be smooth and fluent, the robot needs to estimate the position of handover location in advance during both cases when robot act as receiver or when robot acts as giver, therefore we formulate the motion of subject's hand as a constant velocity based linear motion model (see Algorithm~\ref{positionalgo}) to predict the handover position continuously. The robot observes subject's hand for a predefined time period $t_{observe}$ and estimates the subject's hand direction and position based on the subject's hand velocity using equations (\ref{predictVel}) and (\ref{predict}) during the predefined time period. 

\begin{equation} \label{predictVel}
{}^{h}\mathcal{\bar{V}}_{M} = \frac{1}{t_{observe}}{\sum_{j=1}^{j=t_{observe}} f'({}^{h}\mathcal{P}_{M}(j)/dt }
\end{equation}


\begin{equation} \label{predict}
{}^{h}\mathcal{P}_M(t_{predict}) = {}^{h}\mathcal{\bar{V}}_{M} \cdot t_{predict}  + {}^{h}\mathcal{P}_{M}(t_{observe})
\end{equation}

\paragraph*{}
The prediction model then updates and converges itself over time and in doing so update the robot's end-effector position towards the subject's hand, upon condition if updated position is within the end-effector's constraint workspace, see Fig XXX.

\paragraph*{}
Using equations~\ref{rotationmatrix} and~\ref{predict} we get the orientation and translation components of ${}^{h}{X}_M$ in~\ref{X_M_h}, and recalling equations~\ref{X_ef_h1} and~\ref{X_ef_h2}, the updated translation component of pl\"ucker transformation ${}^{h}{X}_{ef}$, which provides the predicted position of subject's hand with respect to robot end-effector in the robot coordinate system is given by~\ref{X_ef_ht}, the relative orientation component of ${}^{h}{X}_{ef}$ is discussed in next Section~\ref{relOri}.

\begin{equation}\label{X_ef_ht}
{}^{h}{X}_{ef}(t_{predict}) =  {}^{h}{X}_M  {}^{M}{X}_R {{}^{ef}{X}_R}^{-1}
\end{equation}

where, ${}^{M}{X}_R$ is the Pl\"ucker coordinate transform frame $\mathcal{M}$ relative to frame $\mathcal{R}$, if $\mathcal R \neq M$.

\paragraph*{}
Finally, the handover location is estimated as the subject's hand predicted position when the following conditions satisfy (\ref{min_posErr_vel}), that is when the subject's hand velocity is minimum and robot end-effector is closest to the subject's hand.

\begin{equation}\label{min_posErr_vel}
\left.\begin{aligned}
\norm{{}^{h}\mathcal{\bar{V}}_{M}}& <= 1e^{-2} \\
\norm{({}^{h}\mathcal{P}_{ef}(t_{predict}) - {}^{ef}\mathcal{P}_{R})} & <= 1e^{-2}
\end{aligned}
\right\}
\end{equation}



\subsection{Block Diagram: Position Prediction Model*}

% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=yellow!20, 
text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=green!20, 
text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=blue!20, node distance=3cm,
minimum height=2em]

\begin{tikzpicture}[node distance = 3cm, auto]
% Place nodes
\node [block] (init) {initialize model};
\node [cloud, left of=init] (expert) {${}^{ef}{X}_R $};
\node [cloud, right of=init] (system) {${}^{R}{X}_M $};
\node [block, below of=init] (identify) {object relative pose to robot};
\node [block, below of=identify] (evaluate) {evaluate candidate models};
\node [block, left of=evaluate, node distance=3cm] (update) {update model};
\node [decision, below of=evaluate] (decide) {is best candidate better?};
\node [block, below of=decide, node distance=3cm] (stop) {stop};
% Draw edges
\path [line] (init) -- (identify);
\path [line] (identify) -- (evaluate);
\path [line] (evaluate) -- (decide);
\path [line] (decide) -| node [near start] {yes} (update);
\path [line] (update) |- (identify);
\path [line] (decide) -- node {no}(stop);
\path [line,dashed] (expert) -- (init);
\path [line,dashed] (system) -- (init);
\path [line,dashed] (system) |- (evaluate);
\end{tikzpicture}


\subsection{Algorithm: Position Prediction Model}

\begin{algorithm}[H] \label{positionalgo}
	\DontPrintSemicolon
	
	\KwInput{$\mathcal{}^{M}{X}_R, {}^{ef}{X}_R, mocapData$}
	\KwOutput{${}^{h}wp_{ef}$ \tcp*{predicted waypoints}  }
	\KwData{Initial require: $t_{observe}=10ms, t_{predict}=100ms, i=dt=5ms$}
	
	
	\textit{$\textbf{i+=dt}$} \tcp*{increments as per controller run-time (dt)}
	
	\If{$(i\%t_{observe})==0$}
	{
		\For{$j = 1 \textrm{ to } t_{observe} $}
		{
			${}^{h}\mathcal{P}_M= \textit{mocapData}.handMarker(i-t_{observe})+j)$	
		}
		
		${}^{h}\mathcal{\bar{V}}_{M} = \frac{1}{t_{observe}}{\sum_{j=1}^{j=t_{observe}} f'({}^{h}\mathcal{P}_{M}(j)/dt }$\newline 
		
		
		\tcc{predict subject's hand handover position at $t_{predict}$}
		${}^{h}\mathcal{P}_M(t_{predict}) = {}^{h}\mathcal{\bar{V}}_{M} \cdot t_{predict}  + {}^{h}\mathcal{P}_{M}(t_{observe})$ %\newline % \times 0.005
		
	
		${}^{h}{X}_M= \begin{bmatrix} {}^{h}\mathcal{O}_{M} &  {}^{h}\mathcal{P}_M	\end{bmatrix}$ \newline
			
		\tcc{transform handover position relative to robot end-effector}	
		${}^{h}{X}_{ef}(t_{predict}) =  {}^{h}{X}_M  {}^{M}{X}_R {{}^{ef}{X}_R}^{-1}$ \newline
		

		\tcc{\textit{way points between subject hand and robot end-effector handover location}}
		\SetKwFunction{FMain}{generateWp}
		\SetKwProg{Fn}{Function}{:}{}
		\Fn{\FMain{$ {}^{ef}\mathcal{P}_{R}, {}^{h}\mathcal{P}_{ef}(t_{predict}), t_{predict} $}}
		{
			\For{$k = 0 \textrm{ to } t_{predict} $}
			{
				${}^{h}wp_{ef}(k) = [{}^{h}\mathcal{P}_{ef}(t_{predict}) - {}^{ef}\mathcal{P}_{R}] . (\frac{k}{t_{predict}})  + {}^{ef}\mathcal{P}_{R} $ 
			}	
			\textbf{return} $ {}^{h}wp_{ef} $
		}
	}
	\caption{linear prediction model - Position}
\end{algorithm}


\newpage
\section{Relative Orientation Model}\label{relOri}
The translation component of ${}^{ef}{X}_R $ in the equation (\ref{X_R_ef}) is updated based on the predicted position of subject's hand (see Section~\ref{prediction_model}) while the orientation component is updated based on the relative orientation of subject's hand (see Section~\ref{hand_orientation}).

\begin{figure}[h]
	\centering{\includegraphics[width=.7\linewidth]{Chapters/c5-plots/rviz_robot_lt_hand_obj_frame}}
	\caption{Robot HRP2-Kai (left end-effector) holding object with fixed orientation during handover in the robot frame $\mathcal{R}$}
	\label{fig:robot_lt_hand_obj}
\end{figure} 

\paragraph*{}
Since our robot HRP2-Kai lacks conventional anthropomorphic hands, but instead it has gripper alike hands. For relative orientation model to work, we were required to know the fixed initial orientation of the robot end-effector, in which object handover is feasible between human and robot HRP2-Kai, irrespective of the subject's hand orientation, therefore consider a possible  scenario where the robot end-effector orientation is fixed throughout the handover cycle (transfer of object from human to robot and return to human), irrespective of the condition where robot is a giver or receiver. Let ${{}^{efInit}\mathcal{O}_R}$ denote the initial and fixed orientation of the robot end-effector in the robot frame $\mathcal{R}$ during such human robot object handover scenarios as shown in Fig~\ref{fig:robot_lt_hand_obj}, and Fig~\ref{fig:robot_lt_hand_2layers}, where, ${{}^{efInit}\mathcal{O}_R}$ $\subset$ ${{}^{ef}\mathcal{O}_R}$.

\begin{figure}[h]
	\centering{\includegraphics[width=.7\linewidth]{Chapters/c5-plots/rviz_robot_lt_hand_obj-2layer-empty}}
	\caption{Robot HRP2-Kai (left end-effector) fixed orientation during two handover trials in the robot frame $\mathcal{R}$}
	\label{fig:robot_lt_hand_2layers}
\end{figure} 


In order to correctly transform desired subject's hand orientation into robot end-effector frame, we further utilize the equation (\ref{X_ef_ht}). We take advantage of the QP's Orientation Task (see subsection~\ref{orientationTask}) and calculate the task error $e$ between \textit{current} subject's hand and \textit{fixed} robot end-effector orientations,
the resulting orientation from the Orientation Task is the desired robot end-effector orientation relative to the subject's hand orientation, see Fig~\ref{fig:robot_lt_orientations}, therefore, using fixed orientation component  ${{}^{efInit}\mathcal{O}_R}$, of equation (\ref{X_R_ef}) and current subject's hand orientation ${}^{h}\mathcal{O}_M$ from equation (\ref{rotationmatrix}) derived in Section~\ref{hand_orientation}, we further modify equation (\ref{X_ef_ht}).

\begin{figure}[h]
	\centering{\includegraphics[width=1\columnwidth]{Chapters/c5-plots/robot_lt_orientations}}
	\caption{Robot HRP2-Kai (left end-effector) holding object in multiple possible orientations during handover trials in the robot frame $\mathcal{R}$}
	\label{fig:robot_lt_orientations}
\end{figure} 


\begin{gather}\label{X_efinit_hori}
\left[\begin{array}{cc}
{}^{h}\mathcal{O}_{ef} & {}^{h}\mathcal{P}_{ef}
\end{array}\right] =
\left[\begin{array}{cc}
\bf{{}^{h}\mathcal{O}_M} & {}^{h}\mathcal{P}_M
\end{array}\right]
\left[\begin{array}{cc}
{}^{M}\mathcal{O}_R & {}^{M}\mathcal{P}_R
\end{array}\right]
\left[\begin{array}{cc}
\bf{{}^{efInit}\mathcal{O}_{R}} & {}^{ef}\mathcal{P}_{R}
\end{array}\right]^{-1}
\end{gather}


\newpage
\section{Force Control}
Chapter 12
FORCE CONTROL// robot dynamics and control book


we came up we two methods to solve this problem, one highlights simplicity and other highlights efficiency but when used together we get reliability and safest solution possible.


\subsection{Mocap Marker Based}
The HRP2-Kai gripper hands are equipped with 6-axis The object marker pose given by $\mathcal{}^{obj}{X}_M$ in the mocap frame $\mathcal{M}$.





\subsubsection{Finite State Machine}



\subsubsection{Algorithm: Gripper Force Control}

\begin{algorithm}[H]
	\DontPrintSemicolon
	
	\KwInput{$\mathcal{F}_{w}$\tcp*{EF wrist worldWrenchWithoutGravity}}
	\KwOutput{$\mathcal{F}_{pull}, T_{new}$ \tcp*{Pull force, new threshold based on object mass\newline} }
	\textit{$\textbf{i++}$} \tcp*{increments as per controller run-time (5ms)}
	
	\If{subject hand is near robot}
	{
		\tcc{when SUBJECT holds the object}
		\If{$\mathcal{F}_{w}.norm()<1.0 $\tcp*{gripper is empty}}
		{
			$\mathcal{F}_{zero}= \mathcal{F}_{w}$\tcp*{wrench offset}
		}
		\ElseIf{$\mathcal{F}_{w}.norm()>2.0 $}
		{
			$\mathcal{F}_{load}= \mathcal{F}_{w}$\tcp*{wrench with object}
		}
		\tcc{when ROBOT holds the object}
		\SetKwFunction{FMain}{CheckPullForce}
		\SetKwProg{Fn}{Function}{:}{}
		\Fn{\FMain{$axis\forall${x,y,z}}}
		{
			$objectMass = \mathcal{F}_{load}/9.81 $\tcp*{get object mass}
			
			$\mathcal{F}_{inert} = objectMass * efAce  $\tcp*{$efAce$ using gripper markers}
			
			$\mathcal{F}_{pull} = \vert{(\mathcal\vert{{F}}_{w} - \mathcal\vert{{F}}_{inert})}$
			
			$T_{new} = \mathcal{F}_{load} + T_{old}$\tcp*{$T_{old}$ set to min by user}
			
			\If{$\mathcal{F}_{pull} > T_{new}$ {$\forall${x,y,z}}}
			{
				$openGripper$\tcp*{release object}
			}
		}
	}
	\Else
	{
		\If{$(i \% 200)$}
		{
			$\mathcal{F}_{load} = \vert{(\mathcal{F}_{w} - \mathcal{F}_{zero})} $\tcp*{$\forall${x,y,z} average over time}
		}
	}
	\caption{Force Based Gripper Controller}
\end{algorithm}


\newpage
\section{both hands indiviudal- adding another hand}

the switching is based on the function which  uses hysteresis to compute relative position of object between subject's hand and robot end-effectors 

\newpage
\section{both hands together- using hands together}

\subsection{Force Control changes}

\newpage
\section{add a step-walk \& native stablizer}

\newpage
\section{repeat handover with step-walk}

\newpage
\section{experiments}

\newpage
\section{quantitive analysis}

\newpage
\section{Results}

\newpage
\section{Discussion}
%----an idea --have velocity proportional to the distance between robot ef and subj w.r.t. handover location






%the maths of rotation matrix look for relative rotation
%http://www.fastgraph.com/makegames/3drotation/

%page 11, 12  %http://web.cs.iastate.edu/~cs577/handouts/rotation.pdf

%wiki page ... {$https://en.wikipedia.org/wiki/Rotation_formalisms_in_three_dimensions$}

%3D to 2D rotation matrix reduction
%http://www.continuummechanics.org/rotationmatrix.html

%function ==direction-vector-to-rotation-matrix
%https://stackoverflow.com/questions/18558910/direction-vector-to-rotation-matrix

%vector direction link below
%${https://en.wikipedia.org/wiki/Direction_cosine}$

