% ======================================================================= %
%            Bi-directional dual arm object handover
% ======================================================================= %
	
{\color{blue}\chapter{Proactive whole-body object handover}\label{handover chapter}}

\section{Introduction}\label{introduction}


In the previous human-robot interaction studies~\cite{huber2008human, strabala2013toward, shibata1995experimental} and also in our work in Chapter~\ref{more than just co-workers}, we have shown that the human acceptance of the robot co-worker during a task increases when both the appearance and behavior (motion) of the robot are similar to that of humans, specially during an interactive task. Therefore, in this study as well we primarily chose to consider humanoid robot HRP-2Kai as the robot co-worker. 

Up until now we discussed our findings in the domain of \textit{non-physical} human-robot interaction. This final Chapter focuses on the 2$^{nd}$ part of this thesis, where we introduced a novel framework and particularly focused on solving the problem of intuitive and proactive bi-directional object handover between human and a biped humanoid robot dyad using robot Whole-Body Control (WBC) ---which comes under the broad category of \textit{physical} human-robot interaction. In this particular Chapter, we utilized robot's whole-body motion control but within the context of bi-directional object handover between human and humanoid robot co-worker. We took inspiration and insights from the previous state-of-the-art research in the field of object handover between human-human and human-robot dyads in general and formulated our handover problem.

\clearpage

\section{Handover routine}\label{handover routine}

We take similar approach as one mentioned in the~\cite{medina2016human, nemlekarprompt} and treat the object handover between human and humanoid robot co-worker as one stage \textit{cycle} to perform intuitive and responsive handover of an object between them. Each handover \textit{routine} consist of two handover \textit{cycle}s. From onward we will refer the process of object handover between human and humanoid robot as \textit{cycle}, such that object handover from human to humanoid robot is one \textit{cycle} and return of object from humanoid robot to human as another \textit {cycle}. Therefore, it takes two full handover \textit{cycle}s to return the object back to where it started and complete one handover \textit{routine}.

\begin{figure}[htbp]
		\centering{\includegraphics[width=0.8\columnwidth]{plots/c4-plots/halfsit}}
		\caption{Human and humanoid robot starting posture.}
		\label{fig:halfsit}
\end{figure}

We formulate the bi-directional object handover between human and humanoid robot as single action process (namely, a \textit{cycle}) with different human hand poses (positions and orientations) during the object handover and return request along with the human user chosen hand speeds and trajectories to those handover locations from a predefined starting pose. Both human and robot always initiate handover from a starting posture (see Fig.~\ref{fig:halfsit}). We assume that human is ready with the intention to handover the object to robot if he/she is holding the object in his/her hand. At handover location, robot takes the object that is handed by the human in a single action event (and vice-versa). The only predefined condition is that when human carries the object, he/she must hand it seriously to the robot. During 1$^{st}$ \textit{cycle} of handover \textit{routine}, the human carries the object to different handover locations with distinguishable orientations of the object carrying hand and tries to give the object to robot. Similarly, during 2$^{nd}$ \textit{cycle} of handover, human approaches somewhere in the robot reachable workspace with an intention to receive the object from robot using his/her choice of hand orientation and preferred handover location. 


In (Fig.~\ref{fig:handover routine}), we describe the process of our bi-directional handover \textit{routine} using a continuous yet step-by-step approach. We started the handover \textit{routine} to handover the object between human right hand and robot left end-effector, hereafter and until Section (\nameref{both hands individual}) we will formulate the handover problem under this scenario, afterwards we will generalize our one-handed handover \textit{routine} under four possible handover combination scenarios,

\begin{itemize}
    \item \textbf{human right hand $\longleftrightarrow$ robot left end-effector}
    \item human left hand $\longleftrightarrow$ robot left end-effector 
    \item human left hand $\longleftrightarrow$  robot right end-effector
    \item human right hand $\longleftrightarrow$ robot right end-effector
\end{itemize}


Just like during human to human object handover, another thing that we incorporated in our handover \textit{routine} is to give visual tracking like behavior to our robot HRP-2Kai by following an object or a human hand depending on the handover \textit{cycle} using robot's head. We utilized our robot's QP controller (see Section~\nameref{QPController}) and defined a QP based gaze task to follow object during 1$^{st}$ \textit{cycle} of handover and follow the human hand which is approaching to retrieve the object during 2$^{nd}$ \textit{cycle} of handover.

\begin{figure}[htp]
	\centering
	\tikzstyle{line} = [draw, -latex']
		
	\tikzstyle{cloud} = [draw, ellipse,fill=blue!20, text width=6em, text centered, node distance=4cm,  inner sep=0pt, minimum height=2em]
	
	\tikzstyle{startstop} = [rectangle, text centered, rounded corners, minimum width=3cm, minimum height=1cm, draw=black, node distance=4cm, fill=red!30]

	\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=-70,text centered, text width = 1cm,minimum height=1cm, minimum width=2cm, node distance=4cm, draw=black, fill=blue!30]
	
	\tikzstyle{process} = [rectangle, text centered, minimum width=1cm, minimum height=1cm, draw=black, fill=orange!30]

	\tikzstyle{block} = [rectangle, draw, fill=green!20, text width=10em, text centered, node distance=4cm, minimum height=4em]
	
	\tikzstyle{decision} = [diamond, draw, fill=yellow!20, text width=6em, text badly centered, node distance=4cm, inner sep=0pt]

	\begin{tikzpicture}[node distance = 3cm, auto]
		
	\node [process] (or) {1$^{st}$ or 2$^{st}$ \textit{cycle}};

	\node [startstop, above of=or, node distance = 2cm, auto] (start) {start handover routine};	
	
	\node [cloud, left of=or] (subj) {human has object};
	\node [cloud, right of=or] (robot) {robot has object};
	
	\node [block, below of=or, node distance=2cm] (init) {initialize prediction model};
	\node [io, right of=init] (in1) {robot Ef pose};
	\node [io, left of=init] (in2) {human hand pose};
	
	\node [block, below of=init, node distance = 3cm, auto] (relativePos) {observe human hand pose relative to robot Ef pose};
	
	\node [decision, below of=relativePos] (within) {has human hand arrived within robot's reachable workspace?};
	\node [block, left of=within, node distance=5cm, text width=5em] (wait) {wait \& head-track human hand};
	\node [block, below of=within, node distance=4cm] (moveEf) {start moving robot Ef to predicted position};
	
	\node [decision, below of=moveEf, node distance = 4cm] (ifClose) {is human hand close enough to robot?};
	\node [startstop, below of=ifClose, node distance=3cm] (event) {handover object};
	
	% Draw edges
	\path [line] (subj) -- (or);
	\path [line] (robot) -- (or);
	\path [line] (start) -- (or);
	\path [line] (or) -- (init);
	
	\path [line,dashed] (in1) -- (init);
	\path [line,dashed] (in2) -- (init);
	\path [line] (init) --  node {FSM} (relativePos);
	\path [line] (relativePos) -- (within);

	\path [line] (wait) |- (relativePos);
	\path [line] (within) -- node [near start] {no} (wait);
	\path [line] (within) -- node {yes} (moveEf);
	
	\path[line] (moveEf) -- (ifClose);
	\path [line] (ifClose) -- node [near start] {no} (moveEf);
	\path [line] (ifClose) -- node [near start] {yes} (event);
	
	\end{tikzpicture}
	
	\caption{Overview of human humanoid robot bi-directional handover \textit{routine}.}
	\label{fig:handover routine}
\end{figure}

\clearpage

\section{Experimental setup}
We chose our AIST-CNRS-Joint Robotics Laboratory in Tsukuba Japan as the environment to perform our handover \textit{routine} between human and humanoid robot co-worker. Our experiment setup is shown in (Fig.~\ref{fig:halfsit}). Initially we started by having human co-worker and robot co-worker standing comfortably in front of each other at a distance of 1.2 meters from each other. We have always started handover \textit{cycle} by handing over object from human to robot co-worker. The object that needs to be handover was initially either presented to human co-worker by the experimenter or were kept on the nearby table and ready to be grasped by him/her. We assume that human co-worker is ready with the intention to handover object to robot if he/she is holding the object in his/her hand. The whole setup was partially enclosed by movable panels as we didn't wish to perform this experiment under total isolation.


\subsection{Robot}
We used an HRP-2Kai robot~\cite{Kaneko:RAS_ICHR:2015} as the robot co-worker which has two arms and two legs, a head and a torso. HRP-2Kai is a life-size biped humanoid robot which is 154cm tall, has 32 degree-of-freedoms and weighs about 58kg. It is designed and manufactured by the Kawada Industries, Inc in collaboration with AIST under Humanoid Robotics Project (HRP). Both co-workers used their both hands/end-effectors in the experiment.


\subsection{Mocap}

We used motion capture (in short \textit{mocap}) system manufactured by ({\it Motion Analysis Co.,}), to track and get the position data of passive reflective markers. We used in total twenty passive reflective markers during several handover scenarios. Four of these markers were placed on each of the end-effectors of robot co-worker, three markers were placed on the head of the human co-worker to get his/her position in mocap frame, finally three markers were placed on each of the hands of human co-worker and also on the object itself. The markers on the human hands and object were utilized to get their respective position and orientation in the mocap frame and makers on the robot end-effectors were used during object handover and release-return of object from  the robot grippers to human co-worker as a substitute due to lack of haptic feedback, see Section (\nameref{interaction model}) for more details. These passive markers were tracked using 10 kestrel infra-red cameras, each at 200Hz. These mocap markers position data was later utilized using a real-time interface between mocap system (\texttt{cortex}) and \texttt{ROS} called \textit{ros-cortex-bridge}, which was designed to stream and feed these position data to robot controller in real-time.

\subsection{Handover object(s)}

As shown in (Fig.~\ref{fig:objects}), we used three easily distinguishable objects during the one-handed hand handover between the human and the humanoid robot co-worker. We chose these objects because they have different physical structure and mass. These object's mass ranges from $0.22$ kg to $1.1$ kg.


\begin{figure}[pb]
	\centering{\includegraphics[width=0.7\columnwidth]{plots/c4-plots/objects}}
	\caption{Distinguishable shape and mass objects used during one-handed handover between human and humanoid robot co-worker.}
	\label{fig:objects}
\end{figure}


\clearpage

\section{Robot QP controller}\label{QPController}

We used our research group's native multi objective Quadratic Programming (QP)~\cite{ladder-HRP-2Kai} based low-level Whole-Body Control (WBC) framework to govern and optimize the motion of HRP-2Kai humanoid robot. `WBC exploits the full potential of entire floating based robot body and allows interaction with the environment using multi-contact strategies'. It also allows concurrent execution of several tasks at once, for example in this Chapter using WBC, humanoid robot can utilize one or both of its end-effector(s) to grasp and manipulate the object while at the same time takes a step forward or backward, allowing itself to reach the handover location. 

In order to realize human humanoid robot bi-directional object/tool handover study, we introduced several tasks and formulate them in a quadratic fashion so it is conform with QP based controller. The QP enables whole-body control of our HRP2-Kai humanoid robot while respecting both internal and external constraints. To name a few, such constraints encompass joints limits, force and torque limits, contact constraints, stability constraints such as keeping center of mass (CoM) inside the support polygon along with self collision constraints with the environment and itself while generating optimal joint trajectories. Some major constraints that robot must satisfy at each time step during human-robot object handover are mentioned below.


\subsection{QP constraints}\label{QPConstraints}

The QP controller's objective is to compute an acceleration $\ddot{q}$, where $q$ is the robot configuration vector at each time step ($dt$)  to achieve a set of targets or tasks. At each $dt$, the QP is formulated and solved by the LSSOL solver~\cite{gill1986fortran}. The tasks are formulated either linear constraints or quadratic costs~\cite{ladder-HRP-2Kai}. However, we want to solve these tasks in best possible manner as linearization of some tasks may not be feasible or may be conflicting, therefore using least-square approximation, the quadratic cost $c$ corresponding to a task $\mathscr{T}_i =0$ is given by:


%%% Furthermore, the double integration of desired acceleration act as an input to the low-level built-in PD controller of HRP-2Kai

\begin{equation}
c_i(q,\dot{q}, \ddot{q})  = \frac{1}{2}{\norm{ J_i\ddot{q} + \dot{J}_i\dot{q} -\ddot{\mathscr{T}}_i }^2 }
\end{equation}

where $J_i$ the Jacobian matrix of $\mathscr{T}_i$, and the quadratic objective function of QP controller would be,

\begin{equation}\label{qp equation}
\min_{\ddot{q},\tau,f} \; \sum_{i\in O} \omega_i c_i(q,\dot{q},\ddot{q}) + \omega_{f}\lVert f\rVert^2
\end{equation}

which is subject to constraint on equation of motion, as well as other below mentioned constraints,

\begin{equation}
	M(q)\ddot{q} + N(q,\dot{q}) =  J_c^T f
\end{equation}

where $f$ is the set of contact forces, $M$ denotes inertial matrix of the robot, $N$ accounts for Coriolis  and gravity effects, $J_c$ is the Jacobian matrix of all points of contact. The QP objective function in equation (\ref{qp equation}) is made of two terms and the tasks $\mathscr{T}_i$ mentioned in next Subsection (\nameref{QPTasks}) are weighted against each other by weight $w_i$ based on their relative importance and priority. While the damping weight $w_f$ ensures the smoothness and uniqueness of solution by keeping Hessian matrix positive definite.

\begin{enumerate}	
	\item Static equilibrium:

	$\underline{\tau} \leq J^i(q_i)^Tf_i - g^i(q_i) \leq \overline{\tau}$
	
	where sub-script or super-script $i$ is the $i$-th robot, $q_i$ is the $i$-th robot configuration vector, $f_i$ is the set of contact forces vector of $i$-th robot, $J$ is the Jacobian matrix of all points of contact forces, and $\overline{\tau}$ and $\underline{\tau}$ are the maximum and minimum steady state torque limits respectively, and g be the gravity constant.
	
	\item Joint limits:
	
	$\underline{q_i} \leq q_i \leq \overline{q_i}$
	
	$\underline{q_i}$ and $\overline{q_i}$ are the lower and upper limits for robot joints.
	
	\item Self collisions:
	
	$\delta(X^i_j(q_i), X^i_k(q_i)) > \epsilon_{jk} \qquad \forall(j,k)\in {\mathscr{I}}^i_{self\; collision}$
	
	$\delta$ is the function of distance, $X^i_j(q_i)$ is the occupied volume by $j$-th body of robot in configuration $q_i$, $\epsilon_{jk}$ is the pair of user select minimum distances ($j$,$k$) and ${\mathscr{I}}^i_{self\;collision}$ is the robot sets of self collision pairs.
	
	\item Environment collision:
	
	$\delta(X^i_j(q_i), X_k) > \epsilon_{jk} \qquad \forall(j,k)\in {\mathscr{I}}^i_{robot\;environment}$
	
	${\mathscr{I}}^i_{robot\;environment}$ is the set of robot environment (object) collision pairs.
	
	\item Contact Constraint: $\mathscr{I}_{contact} = J_i(q) (\dot{q}_{s(k)} - \dot{q}_{p(k)}) = 0$\\
	The objective of this constraint is to maintain a null velocity~\cite{ohwovoriole1980externsion} between the joints of bodies in contact, where $p(k)$ and $s(k)$ are the predecessor and successor bodies of robot(s) in contact that imposes a constraint~\cite{featherstone2014rigid}.
\end{enumerate}

%%%	\item acceleration Constraint: $\mathscr{I}_{contact} = J_i(q)\ddot{q}_{s(k)} + \dot{J}(q)_i\dot{q}_{p(k)} = 0$\\


\subsection{QP tasks}\label{QPTasks}

As mentioned earlier, below we introduced following outlined tasks to the multi-objective QP controller to achieve safe and reliable human-robot object handover. Let function $\mathscr{T}_i(q)$ denote the geometric objectives (task error) that we require to regulate to zero or maintain above zero, i.e. $\mathscr{T}_i(q) = 0$ and $\mathscr{T}_i(q) \geq 0$.  

\begin{enumerate}[start=1,label={\arabic*.}]

\item Posture task: $\mathscr{T}_{posture} = q_d - q = 0$\\
Posture task is the first task that is added to the QP controller, it allows the robot to maintain an initial posture.

\item Com task: $\mathscr{T}_{CoM} = CoM_d - CoM(q) = 0$\\
Com task is used in conjunction with the stability constraint to all time maintain the CoM position inside the support polygon. 

\item Joint limit task: $\mathscr{T}_{jlim} = \underline{q_i} \leq q_i \leq \overline{q_i}$\\
This task is used occasionally whenever there is a need to limit certain joint movements.  

\item Position task at body $k$: $\mathscr{T}_{pos} = r^d_k - r_k(q) = 0$\\
Where, $r_k$ and $r^d_k$ are the current and desired position of $i$-th robot $k$-th body. Robot end-effector position task is used to reach the desired handover location~\cite{ladder-HRP-2Kai}.

\item Orientation task at body $k$: $\mathscr{T}_{ori} = Err((E^d_k),  E_k(q))$\\
Where, $E_k$ is the orientation of $i$-th robot body to world. Robot end-effector orientation task is used to compute orientation relative to human hand orientation.

\item Gaze task : $\mathscr{T}_{gaze} =u_{target} - u = 0$\\
Where, $u_{target}$  and $u$ are the target vector in world and body vector that needs to be orient respectively. We used gaze task to orient the head joints of HRP-2Kai towards the active human hand position in world, more details can be found in~\cite{samy2017VecOriTask}. Note that by active human hand we meant by either object carrying human hand in 1$^{st}$ \textit{cycle} or by the hand which is approaching to grasp the object in 2$^{nd}$ \textit{cycle}.

\end{enumerate}


\clearpage

\section{Notations}
Let $\mathcal{M}$ (Mocap) and $\mathcal{R}$ (Robot) be the two fixed frames with same orientation that represent both Cartesian coordinate systems and Pl\"ucker coordinate systems denoted by $X$ in the Euclidean space. Both $\mathcal{M}$ and $\mathcal{R}$ are defined by their position and orientation of a Cartesian frame, such that ${}^MX_R$ denotes the Pl\"ucker coordinate transform which depends only on the position and orientation of frame $\mathcal{M}$ relative to frame $\mathcal{R}$ (see Fig~\ref{fig:frames}).

\begin{figure}[ht]
	\centering{\includegraphics[width=0.5\columnwidth]{plots/c4-plots/frame}}
	\caption{$\mathcal{M}$ and $\mathcal{R}$ Cartesian coordinate systems.}
	\label{fig:frames}
\end{figure}


\begin{itemize}
	\item The mathematical notations in the following sections follows ISO guidelines. The Euclidean distance is represented in meters and angles are represented in degrees. The time unit is set in seconds or mentioned otherwise.
	
	\item We used 6D spatial vectors to express the pose of human hands and robot end-effectors as well as objects. We therefore adopt a 6D notation based on spatial vectors, which was explained in Chapter 2 of~\cite{featherstone2014rigid}.
	
	\item One can get the robot end-effector current orientation ${{}^{ef}\mathcal{O}_R} \in \mathbb{R}^{3\times3}$ and current position ${{}^{ef}\mathcal{P}_R} \in \mathbb{R}^{3}$ respectively in the $\mathcal{R}$ frame, by using QP's \texttt{Orientation} task and \texttt{Position} task (see Subsection~\nameref{QPTasks}), therefore,
	\begin{gather}\label{X_R_ef}
	{}^{ef}{X}_R =
	\left[\begin{array}{cc}
	{}^{ef}\mathcal{O}_R & {}^{ef}\mathcal{P}_R
	\end{array}\right]
	\end{gather}
	
	\item Likewise, ${{}^{h}\mathcal{O}_M} \in \mathbb{R}^{3\times3}$ and ${{}^{h}\mathcal{P}_M} \in \mathbb{R}^{3}$, denotes the human hand $h$ orientation and position respectively in the $\mathcal{M}$ frame, obtained from the $\mathcal{L}$ shape body, which is later further explained in Section (\nameref{hand_orientation}),
	\begin{gather}\label{X_M_h}
	{}^{h}{X}_M =
	\left[\begin{array}{cc}
	{}^{h}\mathcal{O}_M & {}^{h}\mathcal{P}_M \\
	\end{array}\right]
	\end{gather}
	
	\item ${{}^{h}\mathcal{O}_{ef}} \in \mathbb{R}^{3\times3}$ and ${{}^{h}\mathcal{P}_{ef}} \in \mathbb{R}^{3}$ respectively, denotes the human hand $h$ orientation and position relative to robot end-effector,
	\begin{gather}\label{X_ef_h}
	{}^{h}{X}_{ef} =
	\left[\begin{array}{cc}
	{}^{h}\mathcal{O}_{ef} & {}^{h}\mathcal{P}_{ef} \\
	\end{array}\right]
	\end{gather}
	
	\item Let ${{}^{o}\mathcal{O}_M} \in \mathbb{R}^{3\times3}$ and ${{}^{o}\mathcal{P}_M} \in \mathbb{R}^{3}$, denotes the object orientation and position respectively in the $\mathcal{M}$ frame, obtained from another ${\bf L}$ shape body attached on the object.
	\begin{gather}\label{X_M_o}
	{}^{o}{X}_M =
	\left[\begin{array}{cc}
	{}^{o}\mathcal{O}_M & {}^{o}\mathcal{P}_M \\
	\end{array}\right]
	\end{gather}
	
	\item For convenience, we formulated the problem with a common origin {\it O}, such that $\mathcal R \equiv M$ (both frames are located between the feet of robot HRP-2Kai), therefore, we can get human hand pose \texttt{w.r.t.} or relative to robot end-effector
	\begin{equation}\label{X_ef_h1}
	{}^{h}{X}_{ef} = {}^{h}{X}_{M}  {}^{ef}{X_{R}}^{-1}
	\end{equation}
	\item otherwise, when $\mathcal R \neq M$, using Pl\"ucker transform ${}^MX_R$,
	
	\begin{equation}\label{X_ef_h2}
	{}^{h}{X}_{ef} = {}^{h}{X}_{M}  {\bf{}^{M}{X}_R}  {}^{ef}{X_{R}}^{-1}
	\end{equation}
	
\end{itemize}

\clearpage

\section{Position prediction model}\label{prediction_model}

In case of object handover and when the human co-worker is ready with the intention to handover the object, instead of simply waiting for the object to be presented by the human at the handover location, the robot must proactively plan its own movements by observing past and predicting near future human hand movements so that it could arrive at the human chosen handover location approximately at same time ---either to receive or return the object. To make this happen, the estimation of handover time as well as the approximation of handover location must be realized early during the handover \textit{routine}. Note that this proactive nature of the robot co-worker should also be available when the human co-worker requests for the object.

The inputs to the prediction model are robot left end-effector pose $\mathcal{}^{ef}{X}_R$ (eq~\ref{X_R_ef}) in the robot frame and human hand mocap marker position ${}^{h}\mathcal{P}_M$ (eq~\ref{X_M_h}) in the mocap frame of reference. Note that for simplicity, we formulated the problem with a common origin {\it O}, such that frame $\mathcal R \equiv M$. To get the position and orientation of human hand, we placed a rigid body with a shape similar to an alphabet ${\bf L}$ on wrist of human hand(s) and on the object as shown in (Fig.~\ref{fig:lshapes}). The three mocap markers $A, B$ and $C$ make up the three vertices of ${\bf L}$ shape body. Note that from onward by ${}^{h}\mathcal{P}_M$ we would mean the position of point $A$ on the ${\bf L}$ shape body as the human hand marker position (Fig.~\ref{fig:lshapes}).

The prediction model behavior can be tuned by two initially required constant time periods, $t_{observe}$ ---a predefined time period required to observe the motion of human hand and $t_{predict}$ ---required to predict the human hand position in advance.

In order for the handover between human and humanoid robot HRP-2Kai to be smooth and intuitive, the robot needs to proactively estimate the near future position of human hand in advance, during both \textit{cycle}s when robot act as receiver (1$^{st}$ \textit{cycle}) or when robot acts as giver (2$^{nd}$ \textit{cycle}) of a handover \textit{routine}. Therefore we formulate the movements of human hand as a constant velocity based linear motion model to predict his/her hand (hence handover) position continuously. The robot observes human hand movements for a predefined time period $t_{observe}$ along with the average velocity of his/her hand during that time period and then predicts the near future human hand movement direction and position using equations (\ref{predictVel}) and (\ref{predict}) for the predefined  $t_{predict}$ time period.

\begin{equation} \label{predictVel}
{}^{h}\mathcal{\bar{V}}_{M} = \frac{1}{t_{observe}}{\sum_{j=1}^{j=t_{observe}} ({}^{h}\mathcal{P}_{M}(j)-{}^{h}\mathcal{P}_{M}(j-1))/dt }
\end{equation}

\begin{equation} \label{predict}
{}^{h}\mathcal{P}_M(t_{predict}) = {}^{h}\mathcal{\bar{V}}_{M} \cdot t_{predict}  + {}^{h}\mathcal{P}_{M}(t_{observe})
\end{equation}


where $dt$ is controller run-time, in our case it is 5ms as mentioned earlier in the Section (\nameref{QPController}). ${}^{h}\mathcal{\bar{V}}_{M}$ is the average human hand movement velocity during $t_{observe}$ time period. ${}^{h}\mathcal{P}_M(t_{predict})$ is the predicted position of human hand at $t_{predict}$. Later the prediction model then updates and converges itself over time and in doing so updates the robot end-effector position towards the human hand, upon condition if updated position is within the robot end-effector's reachable workspace. 

Recalling equation (\ref{X_ef_h1}) or (\ref{X_ef_h2}), the updated translation component of pl\"ucker transformation ${}^{h}{X}_{ef}$, which provides the predicted position of human hand with respect to robot end-effector in the robot coordinate system is given by (\ref{X_ef_ht}).

\begin{equation}\label{X_ef_ht}
{}^{h}{X}_{ef}(t_{predict}) =  {}^{h}{X}_M  {}^{M}{X}_R {{}^{ef}{X}_R}^{-1}
\end{equation}

where, ${}^{M}{X}_R$ is the Pl\"ucker coordinate transform frame $\mathcal{M}$ relative to frame $\mathcal{R}$, if $\mathcal R \neq M$.


Finally, the handover location is estimated based on the human hand predicted position when the following conditions satisfy (\ref{min_posErr_vel}), that is when the human hand velocity is minimum and robot end-effector is closest to the human hand.

\begin{equation}\label{min_posErr_vel}
\begin{cases}
	\norm{{}^{h}\mathcal{\bar{V}}_{M}}& <= 1e^{-2} \\
	\norm{({}^{h}\mathcal{P}_{M}(t_{predict}) - {}^{ef}\mathcal{P}_{R})} & <= 2e^{-2}
\end{cases}
\end{equation}

where, ${}^{h}\mathcal{P}_{M}(t_{predict})$ is the desired position for robot end-effector to reach and ${}^{ef}\mathcal{P}_{R}$ is the actual current position of robot end-effector and also both positions are in same frame or transformed otherwise. We have  also explained the procedure of predicting human hand position and estimating handover location in Algorithm (\ref{positionalgo}) of (\nameref{handoverAppendix}).

In this section we presented how to predict and estimate the handover location, though just by knowing the handover location in space is not enough for an optimal smooth handover of an object. Therefore in next Section (\nameref{hand_orientation}), we will present the orientation component of ${}^{h}{X}_{ef}$ which gives the graspable relative orientation of robot end-effector \texttt{w.r.t} human hand or object orientation during handover.


\begin{figure}[htpb]
	\centering{\includegraphics[width=.9\columnwidth]{plots/c4-plots/lshpe-lt-rt}}
	\caption{$\mathcal{L}$ shape rigid body on the human hand(s)}
	\label{fig:lshapes}
\end{figure}

\clearpage

\section{Grasp configuration model}\label{hand_orientation}

To keep in mind the comfort and requirement of the human co-worker, it is pivotal for the robot to be able to find the most appropriate configuration to grasp (as \textit{receiver}) or release (as \textit{giver}) the object. Therefore for an intuitive and smooth handover of an object, the robot should relatively orient it's end-effector and configure according to the orientation of either object (1$^{st}$ \textit{cycle}) or human hand (2$^{nd}$ \textit{cycle}) during the handover \textit{routine}. Though there are several possible configurations to handover an object, but it is crucial for the robot to determine the correct configuration of its end-effector during the handover which is suitable and comfortable enough for the human and could be perceived natural in the eyes of human co-worker. Note that in this study, we mainly chose to handover the object in which it is most commonly being grasped hence natural to the eyes of the human, one example would be holding an object such as a water bottle when its cap is in upright position during the handover. 

We propose a simple method to get the desired object grasping orientation of robot end-effector either by considering the relative orientation of the active human hand or the object itself. This handover grasp configuration model consists of two sub-models, in first sub-model, we determine the orientation of the active human hand or object itself. In second sub-model, we utilize the first sub-model to continuously get the desired relative orientation of the robot end-effector during the handover \textit{routine}. 


%%% 1st sub-model

We first started by determining the object or active human hand orientation, as  mentioned in previous section we placed an ${\bf L}$ shape rigid body on the wrist of the human hand(s) and a similar one on the object as well such that during the object handover scenario, vector $\vec{AB}^{\,}$ along the longer side and vector $\vec{BC}^{\,}$ along the shorter side of the ${\bf L}$ shape are set to be parallel with the $X$-axis and $Y$-axis of the mocap frame $\mathcal{M}$ respectively, as well as orthogonal to each other as shown in Fig.~\ref{fig:lshapes}. For simplicity, lets consider $\hat{x}$ be the unit vector, which is parallel and along the $X$-axis and likewise a unit vector $\hat{y}$ is parallel and along the $Y$-axis of the mocap frame $\mathcal{M}$, such that

\begin{equation*}
\hat{x} = \frac{\vec{AB}^{\,}}{\norm{\vec{AB}^{\,}}}
\end{equation*}

\begin{equation*}
\hat{y} = \frac{\vec{BC}^{\,}}{\norm{\vec{BC}^{\,}}}
\end{equation*}

Let ${}^{h}\mathcal{O}_{M}$ in equation (\ref{X_M_h}) be the column rotation matrix representing the orientation of human hand (or object) in the mocap frame $\mathcal{M}$ and since $\hat{x} \in \mathbb{R}^{3}$ and $\hat{y} \in \mathbb{R}^{3}$ are the two orthogonal unit vectors of an ${\bf L}$ shape body on each human hand (Fig~\ref{fig:lshapes}), therefore a cross product of them would result in another unit vector $\hat{z} \in \mathbb{R}^{3}$ which is also orthogonal to both $\hat{x}$ and $\hat{y}$. Also using right-hand rule, one can easily get the direction of the unit vector $\hat{z}$ by given equation (\ref{cross}).

\begin{equation}\label{cross}
\hat{z} = \hat{x} \times \hat{y}
\end{equation}

Furthermore, to represent the orientation of human hand in $\mathbb{R}^{3\times3}$, we used these unit vectors $\hat{x}, \hat{y}$ and $\hat{z}$ as columns of the rotation matrix ${}^{h}\mathcal{O}_{M}$~\cite{evans2001rotations, altmann2005rotations, jia2017rotation} in equation (\ref{rotationmatrix}), such that these unit vectors $\hat{x}, \hat{y}$ and $\hat{z}$ represent human hand (or object) orientation around the $roll-pitch-yaw$ axes respectively.

\begin{equation}\label{rotationmatrix}
{}^{h}\mathcal{O}_{M} = 
\left\{\begin{array}{cccc}
\hat{x.x} & \hat{y.x} & \hat{z.x} \\
\hat{x.y} & \hat{y.y} & \hat{z.y} \\
\hat{x.z} & \hat{y.z} & \hat{z.z}
\end{array}\right\}_{3\times 3}
\end{equation}

Therefore, the pose (\ref{X_M_h}) of active human hand or object can be determined using equations (\ref{predict}, translation) and (\ref{rotationmatrix}, orientation). Recalling again that active human hand is the one that carries the object during 1$^{st}$ \textit{cycle} or the hand which is approaching to grasp the object during 2$^{nd}$ \textit{cycle} of a handover \textit{routine}.

\begin{figure}[htpb]
	\centering{\includegraphics[width=.7\linewidth]{plots/c4-plots/rviz_robot_lt_hand_obj-2layer-empty}}
	\caption{Some possible fixed orientation of HRP-2Kai (left end-effector) during the handover trials in the robot frame $\mathcal{R}$}
	\label{fig:robot_lt_hand_2layers}
\end{figure} 


\begin{figure}[htpb]
	\centering{\includegraphics[width=.7\linewidth]{plots/c4-plots/rviz_robot_lt_hand_obj_frame}}
%	\centering{\includegraphics[width=.7\linewidth]{plots/c4-plots/relaxPos}}
	\caption{HRP-2Kai (left end-effector) holding object with fixed orientation during handover in the robot frame $\mathcal{R}$}
	\label{fig:robot_lt_hand_obj}
\end{figure} 

%%% 2nd sub-model

Since our robot HRP-2Kai lacks conventional anthropomorphic hands, but instead it has gripper (Fig~\ref{fig:robot_lt_hand_2layers}) alike hands~\cite{kaneko2015humanoid, stasse2019overview} mainly to increase the manipulability. In this second sub-model, in order to continuously determine the desired relative orientation (grasping configuration) of robot end-effector during the handover, we were first required to know the fixed initial orientation of the robot end-effector and active human hand, in which object handover is feasible between human and robot co-workers. 

Consequently we considered a initial possible scenario where the robot end-effector and active human hand orientations are fixed throughout the handover \textit{routine} (transfer of object from human to robot and return to human), irrespective of the handover \textit{cycle} where robot is a giver or receiver. Let ${{}^{efInit}\mathcal{O}_R}$ denote the initial and fixed orientation of the robot end-effector in the robot frame $\mathcal{R}$ during such human-robot object handover scenarios as shown in (Fig~\ref{fig:robot_lt_hand_2layers}, and Fig~\ref{fig:robot_lt_hand_obj}). Whereas, let ${{}^{hInit}\mathcal{O}_M}$ be the fixed grasping configuration of human hand determine by the equation (\ref{rotationmatrix}), such that both,

\begin{equation}\label{fixedOri}
\begin{cases}
{{}^{efInit}\mathcal{O}_R}$ $\subset$ ${{}^{ef}\mathcal{O}_R} \\
{{}^{hInit}\mathcal{O}_R}$ $\subset$ ${{}^{h}\mathcal{O}_M}
\end{cases}
\end{equation}

Note that both of these fixed subsets in equation (\ref{fixedOri}) of handover grasping configuration for human and robot co-workers are selected prior to the handover \textit{routine} based on the known physical structural properties of the object. Since by knowing the object shape, it is possible to determine how a human co-worker would initially grasp the object, assuming that particular grasp is what seemed natural and comfortable to the human co-worker. Also as the active human hand (which grasps the object) has ${\bf L}$ shape body on the wrist, therefore prior to initiating the handover \textit{routine} when human co-worker grasps the object, one can find the ${{}^{efInit}\mathcal{O}_R}$ by aligning the vector $\vec{AB}^{\,}$ and $\vec{BC}^{\,}$ of active human hand ${\bf L}$ shape with the global $X$-axis and $Y$-axis respectively.

Now, during the handover \textit{routine} to correctly transform desired human hand orientation into robot end-effector frame, we further utilize the equation (\ref{X_ef_ht}). We take advantage of the QP's \texttt{Orientation} Task~\cite{murray2017mathematical, ladder-HRP-2Kai} (see Subsection~\nameref{QPTasks}) and compute the task error $Err$ between \textit{current} human hand orientation ${{}^{h}\mathcal{O}_M}$ obtained using equation (\ref{rotationmatrix}) and \textit{fixed} robot end-effector orientation ${{}^{efInit}\mathcal{O}_R}$, the resulting orientation from the \texttt{Orientation} Task is the desired robot end-effector orientation relative to the human hand orientation (see (Fig~\ref{fig:robot_lt_orientations}) for few of many possible orientations).

Therefore using fixed orientation component  ${{}^{efInit}\mathcal{O}_R}$ and current human hand orientation ${}^{h}\mathcal{O}_M$ from equation (\ref{rotationmatrix}) derived in first sub-model, we further modify equation (\ref{X_ef_ht}) to get the desired orientation $ {}^{h}\mathcal{O}_{ef} $ of robot end-effector during the handover \textit{routine}.


\begin{gather}\label{X_efinit_hori}
\left[\begin{array}{cc}
\bf{{}^{h}\mathcal{O}_{ef}} & {}^{h}\mathcal{P}_{ef}
\end{array}\right] =
\left[\begin{array}{cc}
\bf{{}^{h}\mathcal{O}_M} & {}^{h}\mathcal{P}_M
\end{array}\right]
\left[\begin{array}{cc}
{}^{M}\mathcal{O}_R & {}^{M}\mathcal{P}_R
\end{array}\right]
\left[\begin{array}{cc}
\bf{{}^{efInit}\mathcal{O}_{R}} & {}^{ef}\mathcal{P}_{R}
\end{array}\right]^{-1}
\end{gather}


\begin{figure}[ht]
	\centering{\includegraphics[width=1\columnwidth]{plots/c4-plots/robot_lt_orientations}}
	\caption{Robot HRP-2Kai (left end-effector) multiple possible object grasping configurations during the handover trials in the robot frame $\mathcal{R}$.}
	\label{fig:robot_lt_orientations}
\end{figure} 

Finally, one can get the pose of the handover location by using the translation component of ${}^{h}{X}_{ef} $ in the equation (\ref{X_ef_h}) which is updated based on the predicted position of active human hand (see Section~\nameref{prediction_model}) while the orientation component is updated based on the relative orientation of object during 1$^{st}$ \textit{cycle} and active human hand during 2$^{nd}$ \textit{cycle} of a handover \textit{routine}.

\clearpage

\section{Interaction forces model}\label{interaction model}

One of the main problem that we focused here is related to the \textit{timing} of grasping and releasing the object and minimizing the forces during such interactions. The \textit{timing} at which the robot should close its gripper(s) in order to grasp the object, if robot tries to close too early to grasp the object when human co-worker is not ready it can lead to collision resulting in an accident or if robot waits too long then it may lead to unreliable behavior. Along with the previously mentioned problem we also address here another issue related with the \textit{magnitude} of interaction forces between human co-worker hand(s) and robot co-worker end-effector(s) holding the object during the release of object in the 2$^{nd}$ \textit{cycle} of handover i.e. when robot returns the object to the human.


We had to find the equilibrium for robot to know the appropriate \textit{timing} at which the handover should occur, which would be intuitive and can be perceived natural to the eyes of human, keeping in mind that HRP-2Kai doesn't have anthropomorphic hands but rather manipulative grippers and at the same time we needed to come up with a solution to keep the \textit{magnitude} of interaction forces at minimum. We came up with two methods to tackle this problem, one highlights simplicity and other highlights efficiency but when used together we get a reliable and safest solution possible under such scenario.


HRP-2Kai is equipped with 6-axis \textit{wrist} force sensor on both hands, capable of precisely sensing interaction forces and torques with the environment along $x, y, z$ axes of the sensor local coordinate frame (let's call it $s$). We designed a model of interaction forces which enable the robot end-effector to interact with the object independent of the knowledge of object's mass in advance.

\subsection{Method 1: mocap markers}

Without actual haptic sensor feedback information from the robot end-effector, we had to rely on the wrist force sensor and mocap markers data of the robot end-effector (see Fig.~\ref{fig:markerEf}), therefore we used mocap makers in conjunction with the force feedback from the wrist sensor of HRP-2Kai robot to know whether the object is within its vicinity to grasp or not. Mocap makers on the robot end-effector were crucial to know the relative position of object from the active human hand holding it and the humanoid robot end-effector. We placed four passive infra-red markers on the robot end-effector(s) in rectangular shape configuration as shown in (Fig.~\ref{fig:markerEf} and~\ref{fig:markerEf2}). Two of the markers were placed parallel and along the local $x$-axis of force sensor on the wrist of robot end-effector and rest two markers were placed on the gripper tips as shown in (Fig.~\ref{fig:markerEf}).

\begin{figure}[ht]
	\centering{\includegraphics[width=.5\linewidth]{plots/c4-plots/markerEf.jpg}}
	\caption{Passive IR markers on the robot HRP-2Kai right end-effector.}
	\label{fig:markerEf}
\end{figure} 


Let \{${}^{wa}\vec P, {}^{wb}\vec P, {}^{ga}\vec P, {}^{gb}\vec P$\} $\in \mathbb{R}^{3}$ be the four position vectors of the mocap markers that are placed on the robot end-effector wrist and gripper tips respectively, also let ${}^{obj}\vec P\in \mathbb{R}^{3}$ be the position vector of mocap marker that placed on the object as shown in the (Fig.~\ref{fig:robot_lt_hand_obj} and Fig.~\ref{fig:robot_lt_orientations}). To get the relative position of object and active human hand with respect to robot end-effector, assuming human has the object and is ready with the intent to handover the object. We utilized basic linear algebra and vector products~\cite{brand1947vector, crowe1994history, artin2016geometric} to get the area of triangles $\triangle{{}^{wa}P {}^{wb}P {}^{ga}P}$, $\triangle{{}^{wa}P {}^{wb}P {}^{gb}P}$ and $\triangle{{}^{wa}P {}^{wb}P {}^{obj}P}$ using equation (\ref{area general}) such that upon the satisfaction of following conditions in equation (\ref{area bool}) would give us the relative position of object. Basically we were interested in the output of function $f({}^{obj}\vec{P})$, this function measures the area of triangles formed by the markers on robot end-effector and the object marker. The positive output means that object is within the graspable reach of robot end-effector and that the robot now is ready to close the gripper.

% https://math.stackexchange.com/questions/738236/how-to-calculate-the-area-of-a-triangle-abc-when-given-three-position-vectors-az

\begin{equation}\label{area general}
        \triangle{ABC} = \frac{\norm{ \vec{AB} \times \vec{AC} }} {2} \\
\end{equation}
where $\vec{A}$, $\vec{B}$, $\vec{C}$ are the three coordinate vectors of a triangle $\triangle{ABC}$,


\begin{equation}\label{area}
    \centering
    f({}^{obj}\vec{P}) = 
    \begin{cases}
     (\triangle{{}^{wa}P {}^{wb}P {}^{ga}P} - \triangle{{}^{wa}P {}^{wb}P {}^{obj}P}) > 0 \quad \bigwedge \\
     (\triangle{{}^{wa}P {}^{wb}P {}^{gb}P} - \triangle{{}^{wa}P {}^{wb}P {}^{obj}P}) > 0
   \end{cases}         
\end{equation}

\begin{equation}\label{area bool}
    \centering
    if
    \begin{cases}
     f({}^{obj}\vec{P})==1, & \text{within graspable reach}  \\
     f({}^{obj}\vec{P})==0, & \text{object outside gripper}
   \end{cases}         
\end{equation}





\begin{figure}[ht]
	\centering{\includegraphics[width=.8\linewidth]{plots/c4-plots/efmarkers}}
	\caption{Passive IR markers on the robot HRP-2Kai right end-effector, human right hand and object during handover.}
	\label{fig:markerEf2}
\end{figure} 

We explained further in detail in the Subsection (\nameref{FSM}), but before that we mention another (\nameref{surface wrench}) that we also utilize for closing gripper when object is in the vicinity.



\subsection{Method 2: surface wrench}\label{surface wrench}

\begin{figure}[ht]
	\centering{\includegraphics[width=0.7\columnwidth]{plots/c4-plots/wrist-surface2}}
	\caption{Virtual intrinsic surfaces (green) on robot wrists.}
	\label{fig:wrist-surface2}
\end{figure}

Just by knowing the relative position of the object with respect to the robot end-effector gripper is not enough for safe and reliable object handover. This is due to the lack of anthropomorphic hands and the visual features of the manipulative gripper can be intimidating to some humans. Therefore in conjunction to the mocap marker based method mentioned earlier we also computed ${}^{surf}\vec{f}$, the \textit{gravity-free} force vector in the intrinsic surface frame $surf$ of the gripper (see Fig.~\ref{fig:wrist-surface2}). Let ${}^s\vec{f}\in F^6$ denote the \textit{gravity-free} spatial force vector in the local sensor frame $s$. Note that ${}^s\vec{f}$ has both force and couple components, but we were only interested in the force component. We can easily get the coordinate transformation between surface frame and force sensor frame, knowing the body at which force sensor is attached.

\begin{equation}\label{X_s_surf}
    {}^{surf}X_{s} = {}^{surf}X_{b} {}^{b}X_s
\end{equation}

where, $b$ denotes the body frame at which the force sensor is attached, in our case force sensor(s) are attached to the wrist(s) of HRP-2Kai. One can generally obtain the transformation matrix for a force vector as explained in~\cite{featherstone2014rigid} if $X$ is responsible for coordinate transformation on motion vectors then using the equation (\ref{X*}), $X^{*}$ would do same transformation on force vectors, since both are related~\cite{featherstone2014rigid},

%%% check page 17-25
\begin{equation}\label{X*}
    X^{*} = X^{-T}
\end{equation}

and therefore, force vector in the surface frame $surf$ can be obtained by utilising equations (\ref{X*}) and (\ref{X_s_surf}), we get

% Dual Coordinates: page 9~\cite{featherstone2014rigid} $X^{*} f$, check page 246 as well
\begin{equation}\label{force surf}
    {}^{surf}\vec{f} = {}^{surf}X_{s}^{*} {}^s\vec{f}
\end{equation}

where, ${}^{surf}X_{s}^{*}$ is the transformation matrix for transformation of force vector from force sensor frame $s$ to gripper intrinsic surface frame $surf$. Finally we measure noise free $\abs{{}^{surf}\vec{f}}$ along the gripper's insertion ($z$)-axis, during the interaction between object and gripper just prior to handover during 1$^{st}$ \textit{cycle} and if the measured $\abs{{}^{surf}\vec{f}}$ is greater than 5N along with the outcome of equation (\ref{area}) then it is safe for robot to close the gripper and hence grasp the object. We further explained complete handover \textit{routine} utilizing above discussed methods in the next Subsection (\nameref{FSM}).

\subsection{Finite state machine}\label{FSM}

\begin{figure}[hpt]
	\centering{\includegraphics[width=1\columnwidth]{plots/c4-plots/h-to-r.jpg}}
	\caption{human to humanoid robot object handover, t1 to t8 transition states.}
	\label{fig:h-to-r}
\end{figure}

The whole handover \textit{routine} is a continuous process but for clarity we have divided it into several transition steps of Finite State Machine (FSM)~\cite{johnson1968automatic}, also (see Fig.~\ref{fig:fsm}) for graphical implementation and Algorithm (\ref{interaction forces}) as well, the transitions between states are set to be continuous upon the success of previous transition. In order to understand the interaction forces model, let $\mathcal{\vec{F}}\in \mathbb{R}^3$ denote the current \textit{gravity-free} force sensor reading in the robot frame $\mathcal{R}$. During 1$^{st}$ \textit{cycle} of human to robot handover when human holds the object and starts to approach somewhere with the reachable workspace of robot to handover the object (see Fig.~\ref{fig:h-to-r}), in such a situation due to any acceleration of the robot end-effector while approaching towards the object, a wrist force sensor would only show readings due to the inertial forces~\cite{spong2008robot} that are acting on the robot end-effector, which we termed ${}^{zero}\vec{\mathcal{F}}$ as force sensor offset reading (see equation~\ref{Fzero}). Let ${}^{obj}\overline{\mathcal{F}}\in \mathbb{R}^3$ be the average of contact forces between the end-effector (gripper) and the environment (object) during the resting (${}^{ef}v=0$) period of robot end-effector along the $xyz$ axes and let ${}^{inert}\vec{\mathcal{F}}$ be the inertial force acting on the object due to the acceleration (lets call it ${}^{ef}a$) of robot end-effector when moving towards the predicted pose of human hand during the 2$^{nd}$ \textit{cycle} of handover as shown in (Fig.~\ref{fig:r-to-h}). Let ${}^{pull}\vec{\mathcal{F}}$ be the current \textit{minimum} interaction force exerted on the object by the human co-worker and sensed by the robot wrist force sensor while retreating the object during 2$^{nd}$ \textit{cycle} of robot to human handover and let ${}^{old}T$, ${}^{new}T$ $\in \mathbb{R}^3$ be the respective initial default hand-tuned and updated force thresholds. We now explain each state of FSM as shown in (Fig.~\ref{fig:fsm}).

%\footnote{\label{appendixC} To get more information on how we transformed force sensor local frame $s$ into robot world frame $\mathcal{R}$ and without gravity free reading please see Appendix~\ref{handover appendix}.}


\begin{figure}[hptb]
	\centering
	\begin{tikzpicture}[node distance=5cm, auto, scale = 1, ->,>=stealth, line width=2.3pt, kant/.style={text width=2cm, text centered, sloped}]
	\tikzstyle{cloud} = [draw,ultra thick, circle ,fill=white!20, text width=4em, text centered, node distance=9em, auto, inner sep=0pt, minimum height=1em]
	\tikzstyle{blank} = [circle ,fill=white!20, text width=5em, text centered, node distance=5em, auto, inner sep=0pt, minimum height=2em]
	\tikzstyle{line} = [draw, -latex', inner sep=0pt, minimum height=2em]
	
	\node[cloud,initial above] (human has object) {human has object?};
	\node[cloud, node distance=4cm] (human hand is near?) [right of=human has object] {is object near robot?};
	\node[cloud] (open gripper) [below of=human hand is near?] {open gripper};
	\node[cloud] (stop motion) [right of=open gripper] {maintain current pose};
	\node[cloud] (object inside gripper?) [below of=open gripper] {object inside gripper?};
	\node[cloud] (close gripper) [right of=object inside gripper?] {close gripper};
	\node[cloud] (robot has object) [right of=close gripper] {robot receives object};		
	\node[cloud] (go rest pose) [below of=robot has object] {go to rest pose};
	\node[cloud] (start motion) [left of=go rest pose] {continue motion};
	\node[cloud, node distance=3.8cm] (human hand is near again?) [left of=start motion] {human hand near object?};	
	\node[cloud] (pull handover object) [below of=human hand is near again?] {human pulls object?};
	\node[cloud, node distance=5cm] (open gripper release object) [right of=pull handover object] {gripper releases object};
	\node[cloud] (handover occurred) [below of=open gripper release object] {human receives object};
	\node[cloud, node distance=5cm] (go rest pose2) [left of=handover occurred] {go to rest pose};
	\node[blank] (blank)[left of = go rest pose2]{};
	
	\path [line] (human has object) edge     node[above, kant]{t1, 1$^{st}$ \textit{cycle}} (human hand is near?);
	\path [line] (human hand is near?) edge      node {t2} (open gripper);
	\path [line] (open gripper) edge              node {t3} (stop motion);
	\path [line] (open gripper) edge              node {t4} (object inside gripper?);
	\path [line] (object inside gripper?) edge   node {t5} (close gripper);
	\path [line, dashed] (close gripper) edge          node [above, kant]{t6, if false close?} (open gripper);
	\path [line] (close gripper) edge         node {t7}(robot has object);
	\path [line] (robot has object)edge        node[left]{t8, ${}^{obj}F$} (go rest pose);
	\path [line] (go rest pose) edge           node [above, kant]{t9, 2$^{nd}$ \textit{cycle}} (start motion);
	\path [line]  (start motion)edge           node [above, kant]{t10, ${}^{inert}F$} (human hand is near again?);
	\path [line]  (human hand is near again?)edge  node {t11, ${}^{pull}F, {}^{new}T$}  (pull handover object);
	\path [line]  (pull handover object)edge       node [below, kant]{t12, ${}^{pull}F >{}^{new}T$} (open gripper release object);
	\path [line]  (open gripper release object)edge  node {t13}(handover occurred);
	\path [line]  (handover occurred)edge           node {t14}(go rest pose2);
	\path [line, dashed]  (go rest pose2) -|       node [near end, above, rotate=90]{t0, \textbf{restart handover}}(human has object);
	
	\end{tikzpicture}
	\caption{Overview of human humanoid robot object handover Finite-State-Machine (FSM)}
	\label{fig:fsm}
\end{figure}


\begin{figure}[hpt]
	\centering{\includegraphics[width=1\columnwidth]{plots/c4-plots/r-to-h.jpg}}
	\caption{humanoid robot to human object handover t9 to t14 transition states.}
	\label{fig:r-to-h}
\end{figure}

\begin{enumerate}[start=0,label={\bf{t}\arabic*:}]

    \item $\norm{{}^h{P} - {}^{obj}{P}} < 0.02$\\
    We start FSM under the assumption that object is already being grasped by the human co-worker and he/she is ready with the intention of handover. ${}^h{P}$ is the position of point $A$ on the ${\bf L}$ shape body as the human hand marker position (see Fig.~\ref{fig:lshapes}).
    
    
    \item $\norm{{}^h{P} - {}^{ef}{P}} < 0.10$\\
	During 1$^{st}$ \textit{cycle} of handover \textit{routine}, i.e. human to robot object handover. We measure the relative position of human hand and robot end-effector mocap markers, the state transition is successful if the euclidean distance $d$ is less than 0.1 meters, where $d = \norm{{}^h{P} - {}^{ef}{P}}$. ${}^{ef}{P}$ is the average position of gripper tips marker ${}^{ga}P, {}^{gb}P$. 
    
    \item Open Gripper\\
    Robot opens the gripper and presents with its intention to grasp the object.
    
    \item $\norm{{}^h{P} - {}^{ef}{P}} \leq 0.05$, maintain current pose\\
    To avoid collision between robot end-effector and human hand we reduce the end-effector velocity (${}^{ef}v\simeq0$) when the interaction distance $d = \norm{{}^h{P} - {}^{ef}{P}}$ is less than 0.05 meters. We also measure ${}^{zero}\vec{\mathcal{F}}$ as force sensor offset during this time.

    \begin{equation}\label{Fzero}
        {}^{zero}\vec{\mathcal{F}} = \vert{\vec{\mathcal{F}}}
    \end{equation}

    \item ${}^{surf}\vec{f} \geq 5N \quad \bigwedge \quad f({}^{obj}\vec{P})  == 1$\\
    We declare robot is ready to close the gripper when the object is inside gripper if output of equations (\ref{area bool},~\ref{force surf}) satisfy together.
    
    \item Close Gripper\\
    Let ${}^{close}\vec{\mathcal{F}}$ be the measured force reading at the timing of closing gripper.
        
        \begin{equation}\label{Fclose}
        {}^{close}\vec{\mathcal{F}} = \vert{\vec{\mathcal{F}}}
        \end{equation}
    
    Robot closes gripper, presumably object is grasped as well. However it is easy to check if the object is really grasped by robot or if its a false close. It is safe to say that its a false close if output of equation (\ref{area bool}) is $0$, along with the condition $\norm{{}^{zero}\vec{\mathcal{F}}-{}^{close}\vec{\mathcal{F}}} \simeq{0}$, since these are same measured force sensor offsets. Therefore, in such scenario next transition state would be \textbf{t6} to open gripper and repeat, otherwise \textbf{t7}, as shown in (Fig.~\ref{fig:fsm}).
    
    \item Open Gripper due to false close, otherwise,
    
    \item This transition confirms that robot receives the object and now the object \textit{mass} can be calculated based on the forces measured during previous states.
    
    \begin{equation}
        {}^{obj}\overline{\mathcal{F}} = \frac{1}{n}\sum_{i=1}^{i=n} \vert{ (\vert{\vec{\mathcal{F}}} - \vert{{}^{zero}\vec{\mathcal{F}}}) }
    \end{equation}
    
    ${}^{obj}\overline{\mathcal{F}}$, is pure force sensor reading when ${}^{ef}a=0$ and object is grasped by the robot, $n$ is the low-level controller time-step counter that increments every $5$ms (see Section~\nameref{QPController}), before transitioning to next state, we intentionally stay on this state for 1 second i.e. until $i==200$ to measure the average of ${}^{obj}\vec{\mathcal{F}}$ over 200 samples. Finally, object $mass$ can be obtained using equation (\ref{obj mass}).
    
    \begin{equation}\label{obj mass}
        objMass = \norm{{}^{obj}\overline{\mathcal{F}}}/9.80665
    \end{equation}

    \item Go to rest pose \\
    Both human and robot returns to their resting posture, with robot carrying the object. This ends the 1$^{st}$ \textit{cycle} of handover.
    
    \item 2$^{nd}$ \textit{cycle} begins\\
    Robot to human object handover. The robot continues to predict human hand position and moves towards it when the human co-worker approaches somewhere within the robot's reachable workspace. During this time we measure the ${}^{inert}\vec{\mathcal{F}}$ inertial force acting on the object due to the acceleration of end-effector.

    \begin{equation}
        {}^{inert}\vec{\mathcal{F}}  = objMass * {}^{ef}\overline{a}
    \end{equation}

    Where, ${}^{ef}\overline{a}$, is the average acceleration of the robot end-effector.

    \item $\norm{{}^h{P} - {}^{ef}{P}} < 0.05$\\
    Once again, we measure the relative position of human hand and robot end-effector mocap markers, the state transition is successful if the euclidean distance $d = \norm{{}^h{P} - {}^{ef}{P}}$ is less than 0.05 meters. That means human is ready with the intention to grasp the object.
    
    \item Human attempts to retrieve object
    
    \begin{equation}
    {}^{pull}\vec{\mathcal{F}} = \vert{ (\vert{\vec{\mathcal{F}}} - \vert{{}^{inert}\vec{\mathcal{F}}} - \vert{{}^{zero}\vec{\mathcal{F}}}) }
    \end{equation}
    
    \begin{equation}
    {}^{new}\vec{T} = {}^{obj}\overline{\mathcal{F}} + {}^{old}\vec{T}
    \end{equation}
    
    where, ${}^{old}\vec{T}$ was hand tuned after several attempts with multiple objects. We chose to maintain ${}^{old}\vec{T}$ as default $[6, 6, 6]$ N. 
    
    \item Open Gripper, human grasps the object if both below conditions satisfy

    \begin{equation}\label{Fpull}
    \begin{cases}
     \norm{{}^h{P} - {}^{obj}{P}} < 0.02 \quad \bigwedge  \\
     {}^{pull}\vec{\mathcal{F}}\geq {}^{new}\vec{T}, & \lor x, \lor y, \lor z
   \end{cases}
   \end{equation}

    where, $\norm{{}^h{P} - {}^{obj}{P}}$ is again euclidean distance between human hand and object mocap markers.

    \item Object returns to human co-worker, human retreats.
    
    \item End of 2$^{nd}$ \textit{cycle} of handover.\\
    Both human and robot returns to their resting posture, with human carrying the object (see Fig.~\ref{fig:halfsit}).
    
\end{enumerate}

Finally, this ends the object handover \textit{routine} between human and robot co-workers. Afterwards, we again repeat the handover \textit{routine}, starting with 1$^{st}$ \textit{cycle} of handover.


\clearpage

\section{Either hand generalized handover}\label{both hands individual}

As mentioned in earlier Section (\nameref{handover routine}), up till now we have discussed human-robot bi-directional object handover under the scenario where robot always uses its left end-effector and human always uses his/her right hand. However unlike several handover studies in the past~\cite{cakmak2011human, medina2016human, huber2008Indus, kupcsik2016learning}, we can take the advantage of having a humanoid robot HRP-2Kai at our disposal, therefore we further generalize our human humanoid robot bi-directional object handover \textit{routine} and extend it by exploiting either human left or right hand and similarly left or right end-effector of robot. Basically we extended our one-hand handover \textit{routine} into all four possible scenarios (see Fig.~\ref{fig:handover scenarios}) such as,

\begin{enumerate}
	\item robot left end-effector $R_{efl}$ $\longleftrightarrow$ human right hand $H_r$
	\item robot left end-effector $R_{efl}$ $\longleftrightarrow$ human left hand $H_l$
	\item robot right end-effector $R_{efr}$ $\longleftrightarrow$ human left hand $H_l$
	\item robot right end-effector $R_{efr}$ $\longleftrightarrow$ human right hand $H_r$
\end{enumerate}


\begin{figure}[hpt]
	\centering
	\begin{tikzpicture}[node distance=5cm, auto, scale = 1, ->,>=stealth, line width=2.3pt, kant/.style={text width=2cm, text centered, sloped}]
		\tikzstyle{cloud} = [draw,ultra thick, circle ,fill=white!20, text width=4em, text centered, node distance=9em, auto, inner sep=0pt, minimum height=1em]
		\tikzstyle{line} = [draw, -latex', inner sep=0pt, minimum height=2em]
		
		\node[cloud] (obj) {object};
		\node[cloud] (hl) [below left of = obj] {$H_l$};
		\node[cloud] (hr) [below right of= obj] {$H_r$};

		\node[cloud] (rr) [below of = hl] {$R_{efr}$};
		\node[cloud] (rl) [below of= hr] {$R_{efl}$};

		\path [line] (obj) edge  node[above, kant]{} (hl);
		\path [line] (obj) edge  node[above, kant]{} (hr);		
		
		\path [line] (hl) edge  node[above, kant]{} (rl);
		\path [line] (rl) edge  node[above, kant]{} (hl);
	
		\path [line] (hl) edge  node[above, kant]{} (rr);
		\path [line] (rr) edge  node[above, kant]{} (hl);
	
		\path [line] (hr) edge  node[above, kant]{} (rl);
		\path [line] (rl) edge  node[above, kant]{} (hr);
		
		\path [line] (hr) edge  node[above, kant]{} (rr);
		\path [line] (rr) edge  node[above, kant]{} (hr);

	\end{tikzpicture}
	\caption{Possible handover scenarios between human and humanoid robot.}
	\label{fig:handover scenarios}
\end{figure}


In this section we will only mention changes that need to be incorporated into the previous sections for the generalization and extension of previously stated handover \textit{routine} to cover above four possible scenarios (Fig.~\ref{fig:handover scenarios}). We mainly modify here parameters of FSM states during transitions {\bf t0, t1} and {\bf t10}, which are already discussed in detail (see Subsection~\nameref{FSM}). Once again we have assumed that human is ready with the intention to handover the object if he/she is holding it in his/her either hand. We have also assumed that just like many humans, our robot also acts like a \textit{right-handed} `person', therefore robot right end-effector would get priority over left in case object is at relatively same distance from both end-effectors or in case where the handover predicted position is at or converging towards the center of robot body~\cite{han2013quantifying}.
%, this is mainly because recent study has shown that right hand human tend to prefer right hand over left


In this framework, the choice or preference of employing end-effector (either left or right) by the robot co-worker is based on the shortest relative distance of object to either human hand, which can be determined by using equation (\ref{human obj dist}) along with together the direction (mainly along $y$-axis) and shortest relative distance of \textit{that} active human hand with respect to both end-effectors, as per equations (\ref{human hand dir} and~\ref{robot obj dis}) respectively. For example, if human is holding object in his/her right hand ${}^{hr}{P}$ and if the estimated predicted position of his/her hand (see Section~\nameref{prediction_model}) is converging somewhere in the negative $y$-coordinate space, then robot right end-effector ${}^{efr}{P}$ would be utilized to receive the object during handover as shown in (Fig.~\ref{fig:hr-to-rr}).


\begin{figure}[hpt]
	\centering{\includegraphics[width=0.7\columnwidth]{plots/c4-plots/hr-rr}}
	\caption{Object handover between human right hand and humanoid robot right end-effector.}
	\label{fig:hr-to-rr}
\end{figure}


\begin{equation}\label{human obj dist}
\centering
\begin{cases}
	\norm{{}^{hl}{P} - {}^{obj}{P}} < \norm{{}^{hr}{P} - {}^{obj}{P}}, & \text{human left hand (${}^{hl}{P}$)}  \\
	\\
	\norm{{}^{hl}{P} - {}^{obj}{P}} > \norm{{}^{hr}{P} - {}^{obj}{P}}, & \text{human right hand (${}^{hr}{P}$)}
\end{cases}         
\end{equation}


\begin{equation}\label{human hand dir}
\centering
\begin{cases}
\norm{{}^{h}{P}(y)} > 0.1,  &  \text{robot left end-effector (${}^{efl}{P}$)} \\
\\
\norm{{}^{h}{P}(y)} \leq 0.1,  &  \text{robot right end-effector (${}^{efr}{P}$)}
\end{cases}         
\end{equation}

\begin{equation}\label{robot obj dis}
\centering
\begin{cases}
\norm{{}^{h}{P} - {}^{efl}{P}} < \norm{{}^{h}{P} - {}^{efr}{P}}, &  \text{robot left end-effector (${}^{efl}{P}$)} \\
\\
\norm{{}^{h}{P} - {}^{efl}{P}} > \norm{{}^{h}{P} - {}^{efr}{P}}, &  \text{robot right end-effector (${}^{efr}{P}$)}
\end{cases}
\end{equation}


Where ${}^{h}{P}$ in equations (\ref{human hand dir} and~\ref{robot obj dis}) could be either of the human hand position depending upon equation (\ref{human obj dist}). However in cases where there is a switching of object in between human hands and within the 1$^{st}$ \textit{cycle} of handover, such as in some rare case human co-worker may decide to move the object from his/her left to right hand or vice-versa for any reasons, under those conditions we rely on the effect of \textit{hysteresis} for robot to decide whether it needs to switch end-effector or continue uninterruptedly. But note that we do not consider the problem of object handover in-between robot end-effectors, therefore once the object is being handed over to the robot co-worker i.e. during 2$^{nd}$ \textit{cycle}, then the robot would not be able to switch its end-effector, however the human co-worker is still free to choose either of his/her hand to grasp the object back. Using earlier example where human co-worker right hand has the object and his/her position is converging somewhere in the negative $y$-coordinate space. While during the 1$^{st}$ \textit{cycle} of handover if human switches the object in-between his/her hand ---say from right to left hand, then by taking recent history (direction) of human hand into account we further utilize the outputs of equations (\ref{human hand dir} and~\ref{robot obj dis}) to measure change in the direction of human hand and its shortest relative distance from the end-effectors, and based on these observations robot decides to act accordingly. By exploiting \textit{hysteresis} effect we make sure that robot doesn't respond abruptly to changes made by human.


\clearpage

\section{Two hands handover}\label{both hands together}

It is very natural between humans to use both hands to manipulate a heavy or large shape object to gain confidence and maintain stability during a physical interaction or even while performing a collaborative task. Using both hands together during the course of handing over such object to one another is no different. Similarly, in scenarios where use of single hand is not enough or feasible to perform safe and reliable handover of a heavy large object between human and humanoid robot workers, then it should also be an obvious choice for robot as well to use both hands together whenever necessary. 


We again take the advantage of having a humanoid robot at our disposal and extend the handover \textit{routine} under the scenario of large object transfer between human and humanoid robot co-workers. We formulate this handover \textit{routine} in a manner such that human co-worker is allowed to use either or both hands to handover/receive the object to/from robot co-worker. However, robot would always use both hands while receiving and returning of such object, given the physical structural properties of the object.

\subsection{Handover object(s)}


\begin{figure}[hpt]
	\centering{\includegraphics[width=1\columnwidth]{plots/c4-plots/handoverPipe}}
	\caption{Hollow cylinder shaft as object for handover between human humanoid robot.}
	\label{fig:pipe_ex}
\end{figure}


The object(s) (two of them) we chose to handover between human and humanoid robot co-workers are cylindrical in nature (see Fig.~\ref{fig:pipe_ex}). We chose these objects purely for simplicity and demonstration purposes in this study. Though our handover model would practically work on several distinguishable objects as long as object's basic physical structural properties are known, however mass of object is optional. The cylindrical structures we used are hollow yet quite rigid in nature. The inner $r_i$ and outer $r_o$ radii for those two objects are \textins{$0.055$, $0.065$} m and \textins{$0.07$, $ 0.08$} m respectively, lengths $l$ are \textins{$0.90$, $ 0.12$} m, and the mass of objects are \textins{$0.40$, $1.1$} kg, again we don't need to know mass of object in advance as it can be computed during handover \textit{routine} when robot carries the object as already explained earlier in the Subsection~\nameref{FSM}. 


\begin{figure}[hpt]
	\centering{\includegraphics[width=0.8\columnwidth]{plots/c4-plots/robot_has_pipe}}
	\caption{HRP-2Kai using both end-effectors to manipulate handover object based on the human hand relative orientation.}
	\label{fig:pipe_ori}
\end{figure}


Here as well we have placed an ${\bf L}$ shape rigid body at the center of the object as shown in (Fig.~\ref{fig:pipe_ex} and Fig.~\ref{fig:pipe_ori}) with respective $xyz$ local coordinate axes in local frame $o$ and are along the direction of robot frame axes. The object pose (position and orientation) is given by ${}^{o}{X}_M$ (see equation~\ref{X_M_o}). Also object orientation can be formulated similarly to human hand orientation, as explained earlier in Section (\nameref{hand_orientation}).


\subsection{Constraint motion}

Following is an overview of both hands constraint handover motion during handover \textit{routine}.
\begin{compactitem}
	\item Human moves with the intention to handover the object.
	\item Both robot end-effectors approaches towards the object.
	\item Robot receives the object.
	\item Contacts are established between robot grippers and object surfaces.
	\item Robot end-effectors motion are governed by the contact constraint under null velocity.
	\item Robot end-effectors follows the dominant human hand under the contact constraint.
\end{compactitem}

Here we approach differently during the 1${}^{st}$ and 2${}^{nd}$ \textit{cycle}s of handover. During the 1${}^{st}$ \textit{cycle} of object handover from human to humanoid robot, we predict and estimate both human hands positions individually along with estimating the object orientation. Specifically, we use unique \texttt{position} and \texttt{orientation} tasks (see Subsection~\nameref{QPTasks}) to move each of the robot end-effector to the desired handover location. Once the object is handed over to the robot co-worker and after grasping it using both end-effectors, we remove the \texttt{position} and \texttt{orientation} tasks on one of the end-effector at the beginning of 2${}^{nd}$ \textit{cycle} because of the kinematic contact constraints (see Subsection~\nameref{QPConstraints}) imposed by the object. 

Now we would first introduce contact surfaces and kinematic constraints before explaining the simultaneous motion of the robot end-effectors due to constraints imposed by the object during 2$^{nd}$ \textit{cycle} of handover \textit{routine}.

\begin{figure}[hptb]
	\centering{\includegraphics[width=1\columnwidth]{plots/c4-plots/grippers-intern}}
	\caption{Robot grippers with graspable internal surfaces.}
	\label{fig:gripper-intern}
\end{figure}


\paragraph*{Contact surface(s)} 

The contact surfaces or surface patches were initially defined on the palm of each gripper in advance. Grippers local frame are shown in (Fig.~\ref{fig:gripper-intern}). The contacts are established, when during the handover \textit{routine} the robot grasp the object and its grippers internal surface come in contact with the object's graspable surface. These contacts are necessary to move freely or limit the motion of end-effectors in one or more direction (3 \texttt{x} translation or 3 \texttt{x} rotation)~\cite{bouyarmane2018multi}. Thus robot may lose one or more degrees of freedom (DOF) in motion due to the constraints introduced by the object's body.

%Let $s_l$, $s_r$ be the respective robot left and right gripper surfaces and $s_o$ be the object surface. Let $\phi(q)$ be the function of distance between these contact surfaces.

\paragraph*{Contact kinematics and constraint}

To allow the concurrent motion of the robot end-effectors and the object, contacts are defined as the kinematic constraints in the ~\nameref{QPController}, specifically we used the \textit{fixed} contact mode, as defined by the~\cite{balkcom2002computing}, where all six degrees of freedom are constrained (no translation, no rotation at contact) which restricts and prevents any possibility of sliding motion between the robot end-effectors and the object. Lets consider $b_{l}$, $b_{r}$ and $b_o$ be the respective left, right end-effectors and object bodies and let $v_i$ be the $i$-th body velocity. Let $j_l$ and $j_r$ be the two joints under the kinematic constraint between these bodies. Such that, the relative joint velocities $v_{jl}$ and $v_{jr}$ between these bodies in contact would be given by the set of equations (\ref{joint_vel})~\cite{Featherstone:1987} and therefore the velocity constraints between these bodies would be given by the set of equations (\ref{vel_const}) \cite{ohwovoriole1980externsion} (see also Subsection~\nameref{QPConstraints}). We later introduce this velocity constraint to the \nameref{QPController}.

\begin{equation}\label{joint_vel}
\centering
\begin{cases}
v_{jl} = v_{bl} - v_{bo} \\
v_{jr} = v_{br} - v_{bo} \\
\end{cases}
\end{equation}

\begin{equation}\label{vel_const}
\centering
\begin{cases}
J_{lo}(v_{bl} - v_{bo}) = 0\\
J_{ro}(v_{br} - v_{bo}) = 0\\
\end{cases}
\end{equation}

where, $J_{ik}$ is the Jacobian matrix of all points of contact forces between $i$-th body and $k$-th body.


\paragraph*{1$^{st}$ \textit{cycle}: before object-robot contacts}

Under the handover \textit{routine} scenarios, we initiate handover 1$^{st}$ \textit{cycle} when human holds the object and start approaching somewhere within the reachable workspace of robot. Assuming that he/she is ready with the intention to handover the object to the robot. Human can grasp the object with any possible comfortable orientation using either left/right or both hands simultaneously and any place on the object's surface along its length. The only constraint we put while holding/grasping object was not to occlude the ${\bf L}$ shape body and mocap markers on it (see Fig.~\ref{fig:h-r-pipe-handover} for such possible pose(s)).


\begin{figure}[hptb]
	\centering{\includegraphics[width=0.8\columnwidth]{plots/c4-plots/h-r-pipe-handover}}
	\caption{Object handover between human and humanoid robot using both human hands and both end-effectors.}
	\label{fig:h-r-pipe-handover}
\end{figure}

Compared to one-handed handover scenario which was discussed in previous Section (\nameref{both hands individual}). Here during human to robot co-worker object handover \textit{cycle} instead of just predicting human hand positions individually like earlier, we also measure the euclidean distances of each human hand on the object's surface with respect to the object's center using following set of equations (\ref{hand rel pos on obj}). These distances acts as an offset correction to the predicted positions of human hands, along the length of the object. These offsets are important as they allow robot to find safe and optimum positions on the object's surface to grasp. Also while making sure end-effectors stay quite away from the human hands occupied object's surface (see Fig.~\ref{fig:h-r-pipe-handover}). Moreover, it avoids any possible head-on collision between the end-effectors and the human hands when the robot tries to approach and grasp the object.


Initially we predict and estimate human hand positions by utilizing previously discussed prediction model from Section (\nameref{prediction_model}). Given the object shape and its structural properties (in our case length of cylindrical shaft), the offsets along object's length can be calculated using set of equations (\ref{hand rel pos on obj}) in the object local frame $o$ (see Fig.~\ref{fig:pipe_ex} B) from the center in both directions. This offset is then further transform into robot frame and finally added to already predicted position of the human hands with respect to robot end-effectors using equation (\ref{offset_transform}).

%\begin{figure}[hptb]
%	\centering{\includegraphics[width=1\columnwidth]{plots/c4-plots/pipe-offset}}
%	\caption{Offset correction to the predicted positions of the human hands.}
%	\label{fig:pipe-offset}
%\end{figure}

\begin{equation}\label{hand rel pos on obj}
\centering
\begin{cases}
a = \norm{{}^{obj}{P} - {}^{h}{P}} & \text{}  \\

b = \abs{l/2 - a}  & \text{} \\
\\
{}^\text{off}{d} = \pm 2*a/3 &  \text{if} \; a \geq b \; \bigwedge \; a\leq l/2\\

{}^\text{off}{d} = \mp 2*b/3 &  \text{if} \; a < b \; \bigwedge \; a\leq l/2\\
\end{cases}
\end{equation}

where, ${}^{obj}{P}$ is the center position given by the mocap marker at its center, $a$, $b$ and ${}^\text{off}{d}$ are the euclidean distances, which provides grasped location of human hands on the object surface from its center. Using this crucial information robot can estimate possible points on object's surface which are available to be grasped during the handover. Here, $h$ in ${}^{h}{P}$ represents either left $hl$ or right $hr$ human hand and the sign of ${}^\text{off}{d}$ depends on the choice of left or right human hand (see Fig.~\ref{fig:pipe_ex} B). Let $\hat{u} = \{0, 1, 0\}$ be the free unit vector along the length of object i.e. along the $o_y$-axis of local frame $o$, therefore offset position vector in object frame would be given by

\begin{equation}
	{}^\text{off}{P}_{o} = {}^\text{off}{d} \: \hat{u}  =  \{0, {}^\text{off}{d}, 0\}
\end{equation}


We know for sure that without these offsets, robot end-effectors would inevitably collide with the human hands. Since our prediction model is estimating and predicting the positions of both human hands. Therefore, we need to introduce these offsets in the predicted positions of human hands relative to robot end-effectors. We start by measuring these offsets continuously and from the beginning of handover $\textit{cycle}$. This allows robot to adjust while approaching towards the object depending on the human hands grasped location on the object's surface. We can obtain the modified predicted positions based on these offsets using Pl\"ucker transformation. We transformed offset position ${}^\text{off}{P}_{o}$ from object frame to mocap frame ${}^\text{off}{P}_{M}$ by below equation (\ref{offset_transform}). Please note, above offset correction method is also valid during the 2$^{nd}$ \textit{cycle} of handover when robot holds the object and approaches towards the human decided handover location.


Note here, we considered predicted position of human hand but to relatively orient robot end-effectors, we considered the orientation of object but not the human hand(s). Since in this example of cylindrical object, orientation of human hand and object is similar however to generalize this for other possible distinguishable shape objects, it is crucial to consider object's orientation instead of human hand. As for the other shape objects relative human hand's orientation to grasp the object may not be feasible to perform by the robot. 

%\begin{equation}\label{offset_transform}
%{}^\text{off}{X}_{M} = {}^\text{off}{P}_{o} {}^{o}\mathcal{O}_M
%\end{equation}

\begin{equation}\label{offset_transform}
{}^\text{off}{X}_{M} = 
{}^\text{off}{P}_{o}
 \left[\begin{array}{cc}
{}^{o}\mathcal{O}_M & {}^{h}\mathcal{P}_M(t_{predict})
\end{array}\right]
\end{equation}


where, ${}^{o}\mathcal{O}_M$ provides the orientation of object in the mocap frame (see equation~\ref{X_M_o}) and ${}^{h}\mathcal{P}_M$ is the predicted position of human hand(s). Finally, using equation (\ref{offset_transform}), we can replace translation and orientation components of ${}^{h}{X}_M$ in the equation (\ref{X_ef_ht}) with ${}^\text{off}{X}_{M}$, therefore new pose of the handover location relative to the robot end-effectors would be given by

\begin{equation}
	{}^{h}{X}_{ef}(\text{off}) =  {\bf{}^{\text{\bf off}}{X}_M}  {}^{M}{X}_R {{}^{ef}{X}_R}^{-1}
\end{equation}

Finally at the handover location, we measure the noise free $\abs{{}^{surf}\vec{f}}$ forces along the both gripper's insertion ($z$)-axes during the interaction between object and gripper just prior to handover. Handover occurs when $\abs{{}^{surf}\vec{f}}$ exceeds 5N on both end-effectors along with the set of following conditions in equation (\ref{min_posErr_vel}) satisfy for both robot end-effectors.

%%%%\begin{figure}[htbp]
%%%%	\centering{\includegraphics[width=0.8\columnwidth]{plots/c4-plots/handoverpipeIntern}}
%%%%	\caption{Virtual intrinsic surfaces (green) on robot wrists and grippers.}
%%%%	\label{fig:wrist-surface}
%%%%\end{figure}
%%%
%%%%we measure noise free $\abs{{}^{surf}\vec{f}}$ force along the local $z$-axis of surface frame $surf$ (see Fig.~\ref{fig:wrist-surface}) during the interaction between object and gripper just prior to handover and if the measured force $\norm{{}^{surf}\vec{f}(y)}$ is greater than 2N along with the outcome of equation (\ref{area}) then it is safe for robot to close the gripper and hence grasp the object.
%%%%see Section~\nameref{prediction_model} for more information.

\paragraph*{2$^{nd}$ \textit{cycle}: after object-robot contacts}

So far we have discussed estimating and predicting object handover pose for the robot end-effectors during the 1${}^{st}$ \textit{cycle} of handover \textit{routine}. Up till now, each robot end-effector motion was controlled by its own \texttt{position} and \texttt{orientation} task. But during the 2$^{nd}$ \textit{cycle} of handover \textit{routine} after the contacts have been established between the robot grippers and the object surfaces. Also because of the introduction of kinematic constraints imposed by the object on the end-effectors, it is impossible for the robot to move its end-effectors individually. The successive motion of the end-effectors is therefore now governed by the \texttt{contact constraint} that we introduce to the robot's low-level QP controller. This constraint maintains the contacts between the object and the robot grippers surfaces all the time and the concurrent motion of the end-effectors is made possible by targeting a null velocity constraint with the target objective $\mathscr{I}_{contact}=0$ at the contact joints between the object and the end-effectors, see equation (\ref{vel_const}). The \texttt{contact constraint} is already explained in the beginning of this Section and also in the Subsection~\nameref{QPConstraints}.

As already mentioned earlier in this section, the human co-worker is allowed to use either or both hands to handover/receive the object to/from robot co-worker. Therefore, when the human starts to approach somewhere within the reachable workspace of robot with an intention to receive the object. We utilized equation (\ref{robot_rel dis}) to select the dominant human hand, mainly by measuring the relative distances between the robot end-effectors and the human hands. We check this condition only once in the beginning of the 2$^{nd}$ \textit{cycle} of handover \textit{routine}, after the human hand(s) arrive within the reachable workspace of the robot.

\begin{equation}\label{robot_rel dis}
\centering
\begin{cases}
\norm{{}^{hr}{P} - {}^{efl}{P}} < \norm{{}^{hl}{P} - {}^{efr}{P}}, &  \text{robot left end-effector (${}^{efl}{P}$)} \\
\\
\norm{{}^{hr}{P} - {}^{efl}{P}} > \norm{{}^{hl}{P} - {}^{efr}{P}}, &  \text{robot right end-effector (${}^{efr}{P}$)} \\
\end{cases}
\end{equation}

Where, ${}^{hl}{P}$ and ${}^{hr}{P}$ are the position vectors of the respective left and right human hand. Depending on the choice of dominant human hand, for example if the left human hand is selected then we remove the \texttt{position} and \texttt{orientation} tasks of the left end-effector and therefore the motion of this end-effector is now governed using the \texttt{contact constraint} by targeting the null velocity. By doing this, the robot right end-effector would lead the motion by predicting and estimating the pose of dominant human hand. The problem of predicting and estimating human hand pose can now be treated similarly to the one-handed handover \textit{routine} as mentioned in the Section (\nameref{both hands individual}). Please note, set of equations (\ref{hand rel pos on obj}) is also valid to calculate the offset position of end-effector, when robot predicts and approaches towards the handover location chosen by the dominant human hand. We just need to replace the human hand position vector ${}^{h}{P}$ with the end-effector position vector ${}^{ef}{P}$. Where, $ef$ in ${}^{ef}{P}$ is either left $efl$ or right $efr$ end-effector.


Finally handover occurs when the set of conditions in the equation (\ref{Fpull}) satisfy for both end-effectors (see~\nameref{FSM}). Note that the conditions in the equation (\ref{Fpull}) are valid only if human tries to pull the object from the robot. After the robot grippers release the object, we also remove the null velocity \texttt{contact constraint} between the end-effectors and the object surfaces, and add again the previously removed \texttt{position} and \texttt{orientation} tasks to that end-effector. This leads to the completion of two-handed handover \textit{routine}. 


%%%Here only the robot end-effector relative to the dominant human hand stays in the QP controller and while the other end-effector motion is governed using the \texttt{contact constraint} by targeting null velocity between the object and end-effector. 
%%
%%%Robot's one of the end-effector loses all its six DOF after the contacts with the object are established and the motion of that following end-effector is now governed by the \texttt{contact constraint}, recalling equation.~\ref{vel_const}) with the target objective $\mathscr{I}_{contact}$ to maintain null velocity between the bodies.

\clearpage

\section{Locomotion and handover}

General broad definition of locomotion describes it as an ability to move from one place to another, it could be either walking, running, jumping etc. Though we are mainly interested in the walking locomotion of a \textit{floating} base robot such as our HRP-2Kai in the context of the object handover between human and robot co-workers. To the best of our knowledge, none of the previous work on the human-robot dyad considered object handover and `walking' in a single framework using a \textit{biped} humanoid robot platform such as HRP-2Kai. Therefore in order for the robot to be fully proactive, we believe it is important to consider the possibility of robot taking a step to handover or exchange an object with the human co-worker, in scenarios where a short distance travel is required.

Up till now we formulated our handover problem under the assumption that both human and robot co-worker are stationary on their respective position in the world frame. While on the other hand, next in this section, we further extended our handover problem and added another scenario in which robot takes a step(\textit{s}) to handover or receive the object from the human co-worker based on the interpersonal distance between them. We have tested this scenario during both when human-robot dyad uses either single or both hands simultaneously.

We humans are surprisingly able to predict where our partner would handover an object, often without an explicit communication. Studies have shown that we humans often handover an object at the middle of our interpersonal distances ~\cite{hansen2017human, kato2018humans, kato2019handovers}.


\textit{ Handovers require one individuals hand to enter the peri-personal space of another individual, which is a space around an individuals body that they are known to be protective of [22], [23]studies of peri-personal space has shown that it is affected by an individuals reach [24] [25] and hence we hypothesized that an individuals size and arm length to affect handovers. } from ~\cite{kato2019handovers}\\


\subsection{Step-walk}

Though there are many methods to generate walking pattern for a biped humanoid robot, however in this study, for step-walk locomotion, we primarily chose to adopt walking pattern generator (WPG) based on the Linear Inverted Pendulum Mode (LIPM) which was designed and tested in our group~\cite{ kajita20013d, caron2016humanoids} along with its native stabilizer~\cite{kajita2010biped, caron2018stair}.



5.2.4 Real-time Walking Pattern Generator- paul

\textit{Walking and sitting	into the truck was achieved using a preprogrammed walking pattern and motion}

\textit{To address dynamic gait generation, we used a walking pattern generator based on the preview control and on the Zero-Momentum Point (ZMP) condition, proposed by Kajita [52]. This algorithm uses a preview window to compute the control law from a simplified Linearized Inverted Pendulum Model (LIPM), to compute the current control command from the future state of the system. The core part of this algorithm is to solve a quadratic programming (QP) problem, where the cost function
is to minimize the jerk of the LIPM and to follow an ideal ZMP trajectory. From
this ZMP trajectory defined by the foot prints given as input, the solution of the QP
gives a Center of Mass (CoM) trajectory which is dynamically stable. Finally, the
reference trajectories are integrated in the Stack of Tasks.}

% 5.Self-Stabilization through pHRI future potential ideas --antonio bussy

3.3.3.2 -walking bussy
\textit{We also highlighted that the walking capacities of the robot needed to be improved.
	Indeed, the stabilizer of the walking pattern generator is designed to work for a humanoid
	walking in standalone, a physical contact is seen as an external perturbation and it is certainly
	not designed to work in a sustained physical contact interaction with a human}

don -1.2.1

~\cite{caron2018stair} -- walking and stablization from here
\url{https://scaron.info/}
\cite{caron2016humanoids} walking pattern

\cite{kajita2010biped}


\textbf{since WPG treats physical interactions as disturbance therefore we trigger motion of hands only during the final double support phase.....}: \textit{the robot pushes only during the double support phase (when both feet are
	in contact with the ground), to allow a larger support polygon (and equivalently
	stability margin) during pushing.}

talk about strategy-- which discussed with ganesh earlier on\\
how we plan to achieve this \textbf{Handover task protocol\\} using Step-walk



Each handover \textit{routine} trial, starts from an initial pose (standing still with both hands down). Afterwards, based on hysteresis, we asked participant(s) to move to `starting position', from there, it is up to participant(s), whether he/she chooses to exchange (handover/receive) object from that position or take a step backward to and exchange the object.


%\clearpage
%\section{Measurements and Results}





\clearpage
\section{Discussion}


regarding the orientation -- we need physical structural properties of object and initial possible object grasping configuration of end-effector.


though our method allows us to not stop the motion of end-effector and still being able to handover (both ways) the object, however, if the handover occurs while both human and robot end- effectors are moving then this problem of handover would be extend to the problem of object collaboration and manipulation, which were already being extensively studied in our research group XXXcite don, bussy.
therefore concenterate on solely on the handover problem, we decided to reduce the ef vel during the time of handover. 


Finally, a human-user study will be conducted shortly after further improving the performance of the overall handover framework with locomotion.

%\textit{However, intergender or cultural differences during handovers were not the
%	focus of this research.}
%
%
%\textit{In future studies, we plan to confirm the effectiveness of our proactive model with user study}



\clearpage % end of Handover




%\section{Preliminary study \textit{incomplete**}}
%\subsection{Data analysis}
%\subsection{Results}
%our results from preliminary user study suggests that....
%refer below link for parameters to analyze
%% \url{file:///home/avasalya/Dropbox%20(CNRS-AIST%20JRL)/PhD/Papers/other's%20thesis/thesis-antonio-bussy.pdf}
%
%for questionnaire
%~\cite{huber2009evaluation, bussy2013approche, qin2013effect, weiss2009addressing}
%
%for analysis
%~\cite{li2015predicting} \textit{For each reaching motion trial, the end position
%	prediction performance is measured by the ratio of prediction
%	error to the motion range of that trial. With all the trials from
%	a testing subject, we plot distribution of the prediction error
%	ratio every 10\% of the motion to show how the prediction
%	error ratio changes as the motion develop}



